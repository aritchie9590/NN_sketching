{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Load MNIST Data\n",
    "import numpy as np\n",
    "from layers import *\n",
    "import solver\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                   (0.5, 0.5), (0.5, 0.5))\n",
    "                             ])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=60000, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(trainset, batch_size=10000, shuffle=True)\n",
    "\n",
    "classes = ('0', '1', '2', '3',\n",
    "           '4', '5', '6', '7', '8', '9')\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "X_train_full, y_train_full = dataiter.next()\n",
    "X_train_full = X_train_full.numpy()\n",
    "y_train_full = y_train_full.numpy()\n",
    "X_train_full = X_train_full.reshape(60000, 28*28)\n",
    "\n",
    "n_train = int(np.round(0.9*X_train_full.shape[0]))\n",
    "d = X_train_full.shape[1]\n",
    "\n",
    "X_train = X_train_full[:n_train]\n",
    "X_val = X_train_full[n_train:]\n",
    "y_train = y_train_full[:n_train]\n",
    "y_val = y_train_full[n_train:]\n",
    "data_mnist = {'X_train':X_train, 'X_val':X_val, 'y_train':y_train, 'y_val':y_val}                              \n",
    "                              \n",
    "dataiter = iter(testloader)\n",
    "X_test, y_test = dataiter.next()\n",
    "X_test = X_test.numpy()\n",
    "y_test = y_test.numpy()\n",
    "X_test = X_test.reshape(10000, 28*28)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sketch the training data\n",
    "A = np.random.randn(2000, X_train_full.shape[0])\n",
    "X_train_full_s = A @ X_train_full\n",
    "X_train_s = X_train_full_s[:-2]\n",
    "X_val_s = X_train_full_s[-1]\n",
    "y_train_full_s = A @ y_train_full\n",
    "y_train_s = y_train_full_s[:-2]\n",
    "y_val_s = y_train_full_s[-1]\n",
    "data_mnist_s = {'X_train':X_train_s, 'X_val':X_val_s, 'y_train':y_train_s, 'y_val':y_val_s}\n",
    "d = X_train_full_s.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'num_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-118263e53467>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfc_relu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m fc_relu_model = fc_relu.fc_relu_Classifier(input_dim=d, hidden_dim=None, num_classes=10,\n\u001b[0;32m----> 4\u001b[0;31m                  weight_scale=1e-3, reg=0.0)\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m fc_relu_solv = solver.Solver(fc_relu_model, data_mnist, optim_config = config,\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'num_classes'"
     ]
    }
   ],
   "source": [
    "# fc_relu Classifier - Single Layer\n",
    "import fc_relu\n",
    "fc_relu_model = fc_relu.fc_relu_Classifier(input_dim=d, hidden_dim=None,\n",
    "                 weight_scale=1e-3, reg=0.0)\n",
    "config = {'learning_rate':1e-2}\n",
    "fc_relu_solv = solver.Solver(fc_relu_model, data_mnist, optim_config = config,\n",
    "                  num_epochs=100, batch_size=64, print_every=100)\n",
    "fc_relu_solv.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9274"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test accuracy for the single layer Softmax\n",
    "test_acc = np.sum(np.argmax(softmax_model.loss(X_test),axis=1) == y_test) / len(y_test)\n",
    "test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<softmax.SoftmaxClassifier at 0x103a71438>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fc_relu Classifier - Single Layer - sketched\n",
    "import fc_relu\n",
    "fc_relu_model = fc_relu.fc_relu_Classifier(input_dim=d, hidden_dim=None, num_classes=10,\n",
    "                 weight_scale=1e-3, reg=0.0)\n",
    "config = {'learning_rate':1e-2}\n",
    "fc_relu_solv = solver.Solver(fc_relu_model, data_mnist, optim_config = config,\n",
    "                  num_epochs=100, batch_size=64, print_every=100)\n",
    "fc_relu_solv.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test accuracy for the single layer Softmax\n",
    "test_acc = np.sum(np.argmax(softmax_model.loss(X_test),axis=1) == y_test) / len(y_test)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax Classifier - Double Layer\n",
    "import softmax\n",
    "softmax_double_model = softmax.SoftmaxClassifier(input_dim=d, hidden_dim=500, num_classes=10,\n",
    "                 weight_scale=1e-3, reg=0.0)\n",
    "config = {'learning_rate':1e-2}\n",
    "softmax_double_solv = solver.Solver(softmax_double_model, data_mnist, optim_config = config,\n",
    "                  num_epochs=100, batch_size=64, print_every=1000)\n",
    "softmax_double_solv.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test accuracy for the single layer Softmax\n",
    "test_acc = np.sum(np.argmax(softmax_double_model.loss(X_test),axis=1) == y_test) / len(y_test)\n",
    "test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load MNIST Data\n",
    "import numpy as np\n",
    "from layers import *\n",
    "import solver\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                   (0.5, 0.5), (0.5, 0.5))\n",
    "                             ])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=60000, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(trainset, batch_size=10000, shuffle=True)\n",
    "\n",
    "classes = ('0', '1', '2', '3',\n",
    "           '4', '5', '6', '7', '8', '9')\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "X_train_full, y_train_full = dataiter.next()\n",
    "X_train_full = X_train_full.numpy()\n",
    "y_train_full = y_train_full.numpy()\n",
    "\n",
    "n_train = int(np.round(0.9*X_train_full.shape[0]))\n",
    "\n",
    "X_train = X_train_full[:n_train]\n",
    "X_val = X_train_full[n_train:]\n",
    "y_train = y_train_full[:n_train]\n",
    "y_val = y_train_full[n_train:]\n",
    "data_mnist = {'X_train':X_train, 'X_val':X_val, 'y_train':y_train, 'y_val':y_val}                              \n",
    "                              \n",
    "dataiter = iter(testloader)\n",
    "X_test, y_test = dataiter.next()\n",
    "X_test = X_test.numpy()\n",
    "y_test = y_test.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 4215) loss: 2.302585\n",
      "(Epoch 0 / 5) train acc: 0.084000; val_acc: 0.107000\n",
      "(Iteration 3 / 4215) loss: 2.302544\n",
      "(Iteration 5 / 4215) loss: 2.302611\n",
      "(Iteration 7 / 4215) loss: 2.302609\n",
      "(Iteration 9 / 4215) loss: 2.302663\n",
      "(Iteration 11 / 4215) loss: 2.302543\n",
      "(Iteration 13 / 4215) loss: 2.302670\n",
      "(Iteration 15 / 4215) loss: 2.302504\n",
      "(Iteration 17 / 4215) loss: 2.302415\n",
      "(Iteration 19 / 4215) loss: 2.302510\n",
      "(Iteration 21 / 4215) loss: 2.302834\n",
      "(Iteration 23 / 4215) loss: 2.302573\n",
      "(Iteration 25 / 4215) loss: 2.302246\n",
      "(Iteration 27 / 4215) loss: 2.301934\n",
      "(Iteration 29 / 4215) loss: 2.302640\n",
      "(Iteration 31 / 4215) loss: 2.302099\n",
      "(Iteration 33 / 4215) loss: 2.302522\n",
      "(Iteration 35 / 4215) loss: 2.302997\n",
      "(Iteration 37 / 4215) loss: 2.302938\n",
      "(Iteration 39 / 4215) loss: 2.302199\n",
      "(Iteration 41 / 4215) loss: 2.302564\n",
      "(Iteration 43 / 4215) loss: 2.301960\n",
      "(Iteration 45 / 4215) loss: 2.302035\n",
      "(Iteration 47 / 4215) loss: 2.302271\n",
      "(Iteration 49 / 4215) loss: 2.302438\n",
      "(Iteration 51 / 4215) loss: 2.302210\n",
      "(Iteration 53 / 4215) loss: 2.302468\n",
      "(Iteration 55 / 4215) loss: 2.302385\n",
      "(Iteration 57 / 4215) loss: 2.302511\n",
      "(Iteration 59 / 4215) loss: 2.302927\n",
      "(Iteration 61 / 4215) loss: 2.301627\n",
      "(Iteration 63 / 4215) loss: 2.303040\n",
      "(Iteration 65 / 4215) loss: 2.303199\n",
      "(Iteration 67 / 4215) loss: 2.302200\n",
      "(Iteration 69 / 4215) loss: 2.302428\n",
      "(Iteration 71 / 4215) loss: 2.302291\n",
      "(Iteration 73 / 4215) loss: 2.302586\n",
      "(Iteration 75 / 4215) loss: 2.301501\n",
      "(Iteration 77 / 4215) loss: 2.302032\n",
      "(Iteration 79 / 4215) loss: 2.302259\n",
      "(Iteration 81 / 4215) loss: 2.302255\n",
      "(Iteration 83 / 4215) loss: 2.301847\n",
      "(Iteration 85 / 4215) loss: 2.302910\n",
      "(Iteration 87 / 4215) loss: 2.301777\n",
      "(Iteration 89 / 4215) loss: 2.303437\n",
      "(Iteration 91 / 4215) loss: 2.300545\n",
      "(Iteration 93 / 4215) loss: 2.303001\n",
      "(Iteration 95 / 4215) loss: 2.302208\n",
      "(Iteration 97 / 4215) loss: 2.302402\n",
      "(Iteration 99 / 4215) loss: 2.302709\n",
      "(Iteration 101 / 4215) loss: 2.303557\n",
      "(Iteration 103 / 4215) loss: 2.302009\n",
      "(Iteration 105 / 4215) loss: 2.299898\n",
      "(Iteration 107 / 4215) loss: 2.303200\n",
      "(Iteration 109 / 4215) loss: 2.303258\n",
      "(Iteration 111 / 4215) loss: 2.302246\n",
      "(Iteration 113 / 4215) loss: 2.302800\n",
      "(Iteration 115 / 4215) loss: 2.303364\n",
      "(Iteration 117 / 4215) loss: 2.304187\n",
      "(Iteration 119 / 4215) loss: 2.302099\n",
      "(Iteration 121 / 4215) loss: 2.303476\n",
      "(Iteration 123 / 4215) loss: 2.301735\n",
      "(Iteration 125 / 4215) loss: 2.301901\n",
      "(Iteration 127 / 4215) loss: 2.301364\n",
      "(Iteration 129 / 4215) loss: 2.300651\n",
      "(Iteration 131 / 4215) loss: 2.302047\n",
      "(Iteration 133 / 4215) loss: 2.302725\n",
      "(Iteration 135 / 4215) loss: 2.302147\n",
      "(Iteration 137 / 4215) loss: 2.302423\n",
      "(Iteration 139 / 4215) loss: 2.302399\n",
      "(Iteration 141 / 4215) loss: 2.300901\n",
      "(Iteration 143 / 4215) loss: 2.299995\n",
      "(Iteration 145 / 4215) loss: 2.301679\n",
      "(Iteration 147 / 4215) loss: 2.300246\n",
      "(Iteration 149 / 4215) loss: 2.302950\n",
      "(Iteration 151 / 4215) loss: 2.301969\n",
      "(Iteration 153 / 4215) loss: 2.301330\n",
      "(Iteration 155 / 4215) loss: 2.301149\n",
      "(Iteration 157 / 4215) loss: 2.300878\n",
      "(Iteration 159 / 4215) loss: 2.299661\n",
      "(Iteration 161 / 4215) loss: 2.300868\n",
      "(Iteration 163 / 4215) loss: 2.300680\n",
      "(Iteration 165 / 4215) loss: 2.301004\n",
      "(Iteration 167 / 4215) loss: 2.303473\n",
      "(Iteration 169 / 4215) loss: 2.303797\n",
      "(Iteration 171 / 4215) loss: 2.300997\n",
      "(Iteration 173 / 4215) loss: 2.302755\n",
      "(Iteration 175 / 4215) loss: 2.299995\n",
      "(Iteration 177 / 4215) loss: 2.302464\n",
      "(Iteration 179 / 4215) loss: 2.299319\n",
      "(Iteration 181 / 4215) loss: 2.302863\n",
      "(Iteration 183 / 4215) loss: 2.301192\n",
      "(Iteration 185 / 4215) loss: 2.304173\n",
      "(Iteration 187 / 4215) loss: 2.301449\n",
      "(Iteration 189 / 4215) loss: 2.300470\n",
      "(Iteration 191 / 4215) loss: 2.300150\n",
      "(Iteration 193 / 4215) loss: 2.302695\n",
      "(Iteration 195 / 4215) loss: 2.303335\n",
      "(Iteration 197 / 4215) loss: 2.300655\n",
      "(Iteration 199 / 4215) loss: 2.300894\n",
      "(Iteration 201 / 4215) loss: 2.302905\n",
      "(Iteration 203 / 4215) loss: 2.301482\n",
      "(Iteration 205 / 4215) loss: 2.300181\n",
      "(Iteration 207 / 4215) loss: 2.303413\n",
      "(Iteration 209 / 4215) loss: 2.304467\n",
      "(Iteration 211 / 4215) loss: 2.302639\n",
      "(Iteration 213 / 4215) loss: 2.301970\n",
      "(Iteration 215 / 4215) loss: 2.300332\n",
      "(Iteration 217 / 4215) loss: 2.304443\n",
      "(Iteration 219 / 4215) loss: 2.304028\n",
      "(Iteration 221 / 4215) loss: 2.303678\n",
      "(Iteration 223 / 4215) loss: 2.304078\n",
      "(Iteration 225 / 4215) loss: 2.301400\n",
      "(Iteration 227 / 4215) loss: 2.301505\n",
      "(Iteration 229 / 4215) loss: 2.301015\n",
      "(Iteration 231 / 4215) loss: 2.299323\n",
      "(Iteration 233 / 4215) loss: 2.303205\n",
      "(Iteration 235 / 4215) loss: 2.304326\n",
      "(Iteration 237 / 4215) loss: 2.300869\n",
      "(Iteration 239 / 4215) loss: 2.302076\n",
      "(Iteration 241 / 4215) loss: 2.296235\n",
      "(Iteration 243 / 4215) loss: 2.300923\n",
      "(Iteration 245 / 4215) loss: 2.303348\n",
      "(Iteration 247 / 4215) loss: 2.304072\n",
      "(Iteration 249 / 4215) loss: 2.305631\n",
      "(Iteration 251 / 4215) loss: 2.302849\n",
      "(Iteration 253 / 4215) loss: 2.302611\n",
      "(Iteration 255 / 4215) loss: 2.303349\n",
      "(Iteration 257 / 4215) loss: 2.304089\n",
      "(Iteration 259 / 4215) loss: 2.301127\n",
      "(Iteration 261 / 4215) loss: 2.305690\n",
      "(Iteration 263 / 4215) loss: 2.302176\n",
      "(Iteration 265 / 4215) loss: 2.303725\n",
      "(Iteration 267 / 4215) loss: 2.300726\n",
      "(Iteration 269 / 4215) loss: 2.303214\n",
      "(Iteration 271 / 4215) loss: 2.303152\n",
      "(Iteration 273 / 4215) loss: 2.298083\n",
      "(Iteration 275 / 4215) loss: 2.304966\n",
      "(Iteration 277 / 4215) loss: 2.301455\n",
      "(Iteration 279 / 4215) loss: 2.298160\n",
      "(Iteration 281 / 4215) loss: 2.301089\n",
      "(Iteration 283 / 4215) loss: 2.301791\n",
      "(Iteration 285 / 4215) loss: 2.303027\n",
      "(Iteration 287 / 4215) loss: 2.304177\n",
      "(Iteration 289 / 4215) loss: 2.304204\n",
      "(Iteration 291 / 4215) loss: 2.302558\n",
      "(Iteration 293 / 4215) loss: 2.301164\n",
      "(Iteration 295 / 4215) loss: 2.303381\n",
      "(Iteration 297 / 4215) loss: 2.300459\n",
      "(Iteration 299 / 4215) loss: 2.299710\n",
      "(Iteration 301 / 4215) loss: 2.298815\n",
      "(Iteration 303 / 4215) loss: 2.306464\n",
      "(Iteration 305 / 4215) loss: 2.301946\n",
      "(Iteration 307 / 4215) loss: 2.299236\n",
      "(Iteration 309 / 4215) loss: 2.307521\n",
      "(Iteration 311 / 4215) loss: 2.299076\n",
      "(Iteration 313 / 4215) loss: 2.303224\n",
      "(Iteration 315 / 4215) loss: 2.297564\n",
      "(Iteration 317 / 4215) loss: 2.301651\n",
      "(Iteration 319 / 4215) loss: 2.303349\n",
      "(Iteration 321 / 4215) loss: 2.304105\n",
      "(Iteration 323 / 4215) loss: 2.302184\n",
      "(Iteration 325 / 4215) loss: 2.305185\n",
      "(Iteration 327 / 4215) loss: 2.299449\n",
      "(Iteration 329 / 4215) loss: 2.303669\n",
      "(Iteration 331 / 4215) loss: 2.298461\n",
      "(Iteration 333 / 4215) loss: 2.301944\n",
      "(Iteration 335 / 4215) loss: 2.300724\n",
      "(Iteration 337 / 4215) loss: 2.303959\n",
      "(Iteration 339 / 4215) loss: 2.302571\n",
      "(Iteration 341 / 4215) loss: 2.296595\n",
      "(Iteration 343 / 4215) loss: 2.299668\n",
      "(Iteration 345 / 4215) loss: 2.299820\n",
      "(Iteration 347 / 4215) loss: 2.300089\n",
      "(Iteration 349 / 4215) loss: 2.306242\n",
      "(Iteration 351 / 4215) loss: 2.301690\n",
      "(Iteration 353 / 4215) loss: 2.303171\n",
      "(Iteration 355 / 4215) loss: 2.302682\n",
      "(Iteration 357 / 4215) loss: 2.301540\n",
      "(Iteration 359 / 4215) loss: 2.300903\n",
      "(Iteration 361 / 4215) loss: 2.304111\n",
      "(Iteration 363 / 4215) loss: 2.305449\n",
      "(Iteration 365 / 4215) loss: 2.301451\n",
      "(Iteration 367 / 4215) loss: 2.301729\n",
      "(Iteration 369 / 4215) loss: 2.302456\n",
      "(Iteration 371 / 4215) loss: 2.298240\n",
      "(Iteration 373 / 4215) loss: 2.298361\n",
      "(Iteration 375 / 4215) loss: 2.303732\n",
      "(Iteration 377 / 4215) loss: 2.299715\n",
      "(Iteration 379 / 4215) loss: 2.299220\n",
      "(Iteration 381 / 4215) loss: 2.306028\n",
      "(Iteration 383 / 4215) loss: 2.303830\n",
      "(Iteration 385 / 4215) loss: 2.305341\n",
      "(Iteration 387 / 4215) loss: 2.301569\n",
      "(Iteration 389 / 4215) loss: 2.303372\n",
      "(Iteration 391 / 4215) loss: 2.297999\n",
      "(Iteration 393 / 4215) loss: 2.301080\n",
      "(Iteration 395 / 4215) loss: 2.299377\n",
      "(Iteration 397 / 4215) loss: 2.301023\n",
      "(Iteration 399 / 4215) loss: 2.298968\n",
      "(Iteration 401 / 4215) loss: 2.303085\n",
      "(Iteration 403 / 4215) loss: 2.300019\n",
      "(Iteration 405 / 4215) loss: 2.304224\n",
      "(Iteration 407 / 4215) loss: 2.302536\n",
      "(Iteration 409 / 4215) loss: 2.306135\n",
      "(Iteration 411 / 4215) loss: 2.305668\n",
      "(Iteration 413 / 4215) loss: 2.302038\n",
      "(Iteration 415 / 4215) loss: 2.301832\n",
      "(Iteration 417 / 4215) loss: 2.301519\n",
      "(Iteration 419 / 4215) loss: 2.305405\n",
      "(Iteration 421 / 4215) loss: 2.301418\n",
      "(Iteration 423 / 4215) loss: 2.301406\n",
      "(Iteration 425 / 4215) loss: 2.304913\n",
      "(Iteration 427 / 4215) loss: 2.302541\n",
      "(Iteration 429 / 4215) loss: 2.301811\n",
      "(Iteration 431 / 4215) loss: 2.299382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 433 / 4215) loss: 2.302876\n",
      "(Iteration 435 / 4215) loss: 2.303429\n",
      "(Iteration 437 / 4215) loss: 2.302766\n",
      "(Iteration 439 / 4215) loss: 2.303518\n",
      "(Iteration 441 / 4215) loss: 2.299934\n",
      "(Iteration 443 / 4215) loss: 2.304704\n",
      "(Iteration 445 / 4215) loss: 2.303229\n",
      "(Iteration 447 / 4215) loss: 2.299959\n",
      "(Iteration 449 / 4215) loss: 2.303017\n",
      "(Iteration 451 / 4215) loss: 2.301271\n",
      "(Iteration 453 / 4215) loss: 2.308124\n",
      "(Iteration 455 / 4215) loss: 2.301840\n",
      "(Iteration 457 / 4215) loss: 2.300463\n",
      "(Iteration 459 / 4215) loss: 2.303585\n",
      "(Iteration 461 / 4215) loss: 2.306587\n",
      "(Iteration 463 / 4215) loss: 2.298898\n",
      "(Iteration 465 / 4215) loss: 2.301408\n",
      "(Iteration 467 / 4215) loss: 2.299565\n",
      "(Iteration 469 / 4215) loss: 2.301766\n",
      "(Iteration 471 / 4215) loss: 2.303675\n",
      "(Iteration 473 / 4215) loss: 2.299456\n",
      "(Iteration 475 / 4215) loss: 2.299776\n",
      "(Iteration 477 / 4215) loss: 2.301679\n",
      "(Iteration 479 / 4215) loss: 2.302454\n",
      "(Iteration 481 / 4215) loss: 2.303916\n",
      "(Iteration 483 / 4215) loss: 2.292850\n",
      "(Iteration 485 / 4215) loss: 2.296703\n",
      "(Iteration 487 / 4215) loss: 2.300369\n",
      "(Iteration 489 / 4215) loss: 2.300345\n",
      "(Iteration 491 / 4215) loss: 2.298627\n",
      "(Iteration 493 / 4215) loss: 2.304476\n",
      "(Iteration 495 / 4215) loss: 2.303206\n",
      "(Iteration 497 / 4215) loss: 2.301121\n",
      "(Iteration 499 / 4215) loss: 2.299491\n",
      "(Iteration 501 / 4215) loss: 2.301928\n",
      "(Iteration 503 / 4215) loss: 2.297740\n",
      "(Iteration 505 / 4215) loss: 2.298337\n",
      "(Iteration 507 / 4215) loss: 2.300416\n",
      "(Iteration 509 / 4215) loss: 2.294760\n",
      "(Iteration 511 / 4215) loss: 2.304645\n",
      "(Iteration 513 / 4215) loss: 2.305352\n",
      "(Iteration 515 / 4215) loss: 2.296390\n",
      "(Iteration 517 / 4215) loss: 2.305737\n",
      "(Iteration 519 / 4215) loss: 2.298763\n",
      "(Iteration 521 / 4215) loss: 2.300473\n",
      "(Iteration 523 / 4215) loss: 2.303195\n",
      "(Iteration 525 / 4215) loss: 2.305116\n",
      "(Iteration 527 / 4215) loss: 2.300080\n",
      "(Iteration 529 / 4215) loss: 2.303098\n",
      "(Iteration 531 / 4215) loss: 2.303299\n",
      "(Iteration 533 / 4215) loss: 2.307339\n",
      "(Iteration 535 / 4215) loss: 2.300558\n",
      "(Iteration 537 / 4215) loss: 2.301096\n",
      "(Iteration 539 / 4215) loss: 2.301721\n",
      "(Iteration 541 / 4215) loss: 2.301404\n",
      "(Iteration 543 / 4215) loss: 2.304149\n",
      "(Iteration 545 / 4215) loss: 2.299790\n",
      "(Iteration 547 / 4215) loss: 2.306968\n",
      "(Iteration 549 / 4215) loss: 2.301840\n",
      "(Iteration 551 / 4215) loss: 2.306068\n",
      "(Iteration 553 / 4215) loss: 2.304358\n",
      "(Iteration 555 / 4215) loss: 2.297090\n",
      "(Iteration 557 / 4215) loss: 2.308336\n",
      "(Iteration 559 / 4215) loss: 2.303438\n",
      "(Iteration 561 / 4215) loss: 2.300666\n",
      "(Iteration 563 / 4215) loss: 2.303378\n",
      "(Iteration 565 / 4215) loss: 2.302630\n",
      "(Iteration 567 / 4215) loss: 2.302385\n",
      "(Iteration 569 / 4215) loss: 2.304850\n",
      "(Iteration 571 / 4215) loss: 2.304698\n",
      "(Iteration 573 / 4215) loss: 2.297105\n",
      "(Iteration 575 / 4215) loss: 2.301238\n",
      "(Iteration 577 / 4215) loss: 2.303572\n",
      "(Iteration 579 / 4215) loss: 2.305787\n",
      "(Iteration 581 / 4215) loss: 2.303241\n",
      "(Iteration 583 / 4215) loss: 2.304920\n",
      "(Iteration 585 / 4215) loss: 2.305231\n",
      "(Iteration 587 / 4215) loss: 2.306830\n",
      "(Iteration 589 / 4215) loss: 2.302712\n",
      "(Iteration 591 / 4215) loss: 2.303710\n",
      "(Iteration 593 / 4215) loss: 2.303697\n",
      "(Iteration 595 / 4215) loss: 2.302618\n",
      "(Iteration 597 / 4215) loss: 2.296520\n",
      "(Iteration 599 / 4215) loss: 2.297250\n",
      "(Iteration 601 / 4215) loss: 2.299531\n",
      "(Iteration 603 / 4215) loss: 2.304514\n",
      "(Iteration 605 / 4215) loss: 2.303415\n",
      "(Iteration 607 / 4215) loss: 2.303178\n",
      "(Iteration 609 / 4215) loss: 2.303049\n",
      "(Iteration 611 / 4215) loss: 2.310229\n",
      "(Iteration 613 / 4215) loss: 2.304434\n",
      "(Iteration 615 / 4215) loss: 2.307483\n",
      "(Iteration 617 / 4215) loss: 2.301955\n",
      "(Iteration 619 / 4215) loss: 2.302315\n",
      "(Iteration 621 / 4215) loss: 2.302956\n",
      "(Iteration 623 / 4215) loss: 2.300183\n",
      "(Iteration 625 / 4215) loss: 2.302102\n",
      "(Iteration 627 / 4215) loss: 2.304138\n",
      "(Iteration 629 / 4215) loss: 2.300330\n",
      "(Iteration 631 / 4215) loss: 2.309781\n",
      "(Iteration 633 / 4215) loss: 2.304150\n",
      "(Iteration 635 / 4215) loss: 2.301761\n",
      "(Iteration 637 / 4215) loss: 2.302031\n",
      "(Iteration 639 / 4215) loss: 2.296878\n",
      "(Iteration 641 / 4215) loss: 2.303444\n",
      "(Iteration 643 / 4215) loss: 2.299797\n",
      "(Iteration 645 / 4215) loss: 2.302056\n",
      "(Iteration 647 / 4215) loss: 2.302553\n",
      "(Iteration 649 / 4215) loss: 2.303120\n",
      "(Iteration 651 / 4215) loss: 2.300086\n",
      "(Iteration 653 / 4215) loss: 2.304923\n",
      "(Iteration 655 / 4215) loss: 2.297863\n",
      "(Iteration 657 / 4215) loss: 2.299323\n",
      "(Iteration 659 / 4215) loss: 2.298600\n",
      "(Iteration 661 / 4215) loss: 2.301685\n",
      "(Iteration 663 / 4215) loss: 2.304382\n",
      "(Iteration 665 / 4215) loss: 2.302224\n",
      "(Iteration 667 / 4215) loss: 2.295272\n",
      "(Iteration 669 / 4215) loss: 2.298831\n",
      "(Iteration 671 / 4215) loss: 2.293637\n",
      "(Iteration 673 / 4215) loss: 2.298583\n",
      "(Iteration 675 / 4215) loss: 2.306996\n",
      "(Iteration 677 / 4215) loss: 2.294519\n",
      "(Iteration 679 / 4215) loss: 2.299050\n",
      "(Iteration 681 / 4215) loss: 2.295550\n",
      "(Iteration 683 / 4215) loss: 2.301200\n",
      "(Iteration 685 / 4215) loss: 2.301094\n",
      "(Iteration 687 / 4215) loss: 2.308442\n",
      "(Iteration 689 / 4215) loss: 2.307775\n",
      "(Iteration 691 / 4215) loss: 2.306076\n",
      "(Iteration 693 / 4215) loss: 2.301827\n",
      "(Iteration 695 / 4215) loss: 2.301648\n",
      "(Iteration 697 / 4215) loss: 2.299607\n",
      "(Iteration 699 / 4215) loss: 2.299325\n",
      "(Iteration 701 / 4215) loss: 2.311544\n",
      "(Iteration 703 / 4215) loss: 2.304012\n",
      "(Iteration 705 / 4215) loss: 2.302552\n",
      "(Iteration 707 / 4215) loss: 2.305559\n",
      "(Iteration 709 / 4215) loss: 2.300705\n",
      "(Iteration 711 / 4215) loss: 2.302977\n",
      "(Iteration 713 / 4215) loss: 2.299773\n",
      "(Iteration 715 / 4215) loss: 2.303310\n",
      "(Iteration 717 / 4215) loss: 2.307463\n",
      "(Iteration 719 / 4215) loss: 2.301184\n",
      "(Iteration 721 / 4215) loss: 2.301809\n",
      "(Iteration 723 / 4215) loss: 2.303503\n",
      "(Iteration 725 / 4215) loss: 2.300936\n",
      "(Iteration 727 / 4215) loss: 2.303685\n",
      "(Iteration 729 / 4215) loss: 2.297355\n",
      "(Iteration 731 / 4215) loss: 2.298611\n",
      "(Iteration 733 / 4215) loss: 2.300741\n",
      "(Iteration 735 / 4215) loss: 2.301588\n",
      "(Iteration 737 / 4215) loss: 2.299196\n",
      "(Iteration 739 / 4215) loss: 2.298653\n",
      "(Iteration 741 / 4215) loss: 2.304276\n",
      "(Iteration 743 / 4215) loss: 2.300431\n",
      "(Iteration 745 / 4215) loss: 2.298696\n",
      "(Iteration 747 / 4215) loss: 2.300666\n",
      "(Iteration 749 / 4215) loss: 2.301216\n",
      "(Iteration 751 / 4215) loss: 2.299804\n",
      "(Iteration 753 / 4215) loss: 2.300915\n",
      "(Iteration 755 / 4215) loss: 2.303086\n",
      "(Iteration 757 / 4215) loss: 2.299832\n",
      "(Iteration 759 / 4215) loss: 2.299674\n",
      "(Iteration 761 / 4215) loss: 2.302071\n",
      "(Iteration 763 / 4215) loss: 2.307159\n",
      "(Iteration 765 / 4215) loss: 2.292253\n",
      "(Iteration 767 / 4215) loss: 2.296710\n",
      "(Iteration 769 / 4215) loss: 2.306026\n",
      "(Iteration 771 / 4215) loss: 2.300830\n",
      "(Iteration 773 / 4215) loss: 2.296570\n",
      "(Iteration 775 / 4215) loss: 2.306984\n",
      "(Iteration 777 / 4215) loss: 2.303932\n",
      "(Iteration 779 / 4215) loss: 2.304216\n",
      "(Iteration 781 / 4215) loss: 2.300766\n",
      "(Iteration 783 / 4215) loss: 2.305442\n",
      "(Iteration 785 / 4215) loss: 2.302090\n",
      "(Iteration 787 / 4215) loss: 2.302202\n",
      "(Iteration 789 / 4215) loss: 2.305268\n",
      "(Iteration 791 / 4215) loss: 2.303034\n",
      "(Iteration 793 / 4215) loss: 2.293732\n",
      "(Iteration 795 / 4215) loss: 2.303374\n",
      "(Iteration 797 / 4215) loss: 2.300952\n",
      "(Iteration 799 / 4215) loss: 2.304599\n",
      "(Iteration 801 / 4215) loss: 2.300102\n",
      "(Iteration 803 / 4215) loss: 2.298960\n",
      "(Iteration 805 / 4215) loss: 2.301447\n",
      "(Iteration 807 / 4215) loss: 2.296620\n",
      "(Iteration 809 / 4215) loss: 2.302107\n",
      "(Iteration 811 / 4215) loss: 2.300123\n",
      "(Iteration 813 / 4215) loss: 2.299570\n",
      "(Iteration 815 / 4215) loss: 2.303064\n",
      "(Iteration 817 / 4215) loss: 2.296163\n",
      "(Iteration 819 / 4215) loss: 2.301574\n",
      "(Iteration 821 / 4215) loss: 2.301907\n",
      "(Iteration 823 / 4215) loss: 2.304416\n",
      "(Iteration 825 / 4215) loss: 2.299119\n",
      "(Iteration 827 / 4215) loss: 2.302228\n",
      "(Iteration 829 / 4215) loss: 2.297223\n",
      "(Iteration 831 / 4215) loss: 2.296678\n",
      "(Iteration 833 / 4215) loss: 2.301600\n",
      "(Iteration 835 / 4215) loss: 2.301699\n",
      "(Iteration 837 / 4215) loss: 2.295550\n",
      "(Iteration 839 / 4215) loss: 2.299736\n",
      "(Iteration 841 / 4215) loss: 2.305316\n",
      "(Iteration 843 / 4215) loss: 2.307440\n",
      "(Epoch 1 / 5) train acc: 0.101000; val_acc: 0.115000\n",
      "(Iteration 845 / 4215) loss: 2.301681\n",
      "(Iteration 847 / 4215) loss: 2.305506\n",
      "(Iteration 849 / 4215) loss: 2.300732\n",
      "(Iteration 851 / 4215) loss: 2.303893\n",
      "(Iteration 853 / 4215) loss: 2.300285\n",
      "(Iteration 855 / 4215) loss: 2.302124\n",
      "(Iteration 857 / 4215) loss: 2.299972\n",
      "(Iteration 859 / 4215) loss: 2.304628\n",
      "(Iteration 861 / 4215) loss: 2.301944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 863 / 4215) loss: 2.304407\n",
      "(Iteration 865 / 4215) loss: 2.301068\n",
      "(Iteration 867 / 4215) loss: 2.302261\n",
      "(Iteration 869 / 4215) loss: 2.293822\n",
      "(Iteration 871 / 4215) loss: 2.301630\n",
      "(Iteration 873 / 4215) loss: 2.298402\n",
      "(Iteration 875 / 4215) loss: 2.302653\n",
      "(Iteration 877 / 4215) loss: 2.301059\n",
      "(Iteration 879 / 4215) loss: 2.301398\n",
      "(Iteration 881 / 4215) loss: 2.301616\n",
      "(Iteration 883 / 4215) loss: 2.293333\n",
      "(Iteration 885 / 4215) loss: 2.297493\n",
      "(Iteration 887 / 4215) loss: 2.309139\n",
      "(Iteration 889 / 4215) loss: 2.294902\n",
      "(Iteration 891 / 4215) loss: 2.303632\n",
      "(Iteration 893 / 4215) loss: 2.300380\n",
      "(Iteration 895 / 4215) loss: 2.300605\n",
      "(Iteration 897 / 4215) loss: 2.303422\n",
      "(Iteration 899 / 4215) loss: 2.296341\n",
      "(Iteration 901 / 4215) loss: 2.300260\n",
      "(Iteration 903 / 4215) loss: 2.298931\n",
      "(Iteration 905 / 4215) loss: 2.305488\n",
      "(Iteration 907 / 4215) loss: 2.294607\n",
      "(Iteration 909 / 4215) loss: 2.298834\n",
      "(Iteration 911 / 4215) loss: 2.300195\n",
      "(Iteration 913 / 4215) loss: 2.305132\n",
      "(Iteration 915 / 4215) loss: 2.294240\n",
      "(Iteration 917 / 4215) loss: 2.306176\n",
      "(Iteration 919 / 4215) loss: 2.301863\n",
      "(Iteration 921 / 4215) loss: 2.308372\n",
      "(Iteration 923 / 4215) loss: 2.296753\n",
      "(Iteration 925 / 4215) loss: 2.297171\n",
      "(Iteration 927 / 4215) loss: 2.307363\n",
      "(Iteration 929 / 4215) loss: 2.297750\n",
      "(Iteration 931 / 4215) loss: 2.305965\n",
      "(Iteration 933 / 4215) loss: 2.304847\n",
      "(Iteration 935 / 4215) loss: 2.301490\n",
      "(Iteration 937 / 4215) loss: 2.307370\n",
      "(Iteration 939 / 4215) loss: 2.297269\n",
      "(Iteration 941 / 4215) loss: 2.293337\n",
      "(Iteration 943 / 4215) loss: 2.305400\n",
      "(Iteration 945 / 4215) loss: 2.305537\n",
      "(Iteration 947 / 4215) loss: 2.297877\n",
      "(Iteration 949 / 4215) loss: 2.303273\n",
      "(Iteration 951 / 4215) loss: 2.292863\n",
      "(Iteration 953 / 4215) loss: 2.306197\n",
      "(Iteration 955 / 4215) loss: 2.307079\n",
      "(Iteration 957 / 4215) loss: 2.298043\n",
      "(Iteration 959 / 4215) loss: 2.304486\n",
      "(Iteration 961 / 4215) loss: 2.305515\n",
      "(Iteration 963 / 4215) loss: 2.304396\n",
      "(Iteration 965 / 4215) loss: 2.302582\n",
      "(Iteration 967 / 4215) loss: 2.305068\n",
      "(Iteration 969 / 4215) loss: 2.301425\n",
      "(Iteration 971 / 4215) loss: 2.306368\n",
      "(Iteration 973 / 4215) loss: 2.311828\n",
      "(Iteration 975 / 4215) loss: 2.305438\n",
      "(Iteration 977 / 4215) loss: 2.298449\n",
      "(Iteration 979 / 4215) loss: 2.309226\n",
      "(Iteration 981 / 4215) loss: 2.306794\n",
      "(Iteration 983 / 4215) loss: 2.291719\n",
      "(Iteration 985 / 4215) loss: 2.296599\n",
      "(Iteration 987 / 4215) loss: 2.296925\n",
      "(Iteration 989 / 4215) loss: 2.308384\n",
      "(Iteration 991 / 4215) loss: 2.304019\n",
      "(Iteration 993 / 4215) loss: 2.303952\n",
      "(Iteration 995 / 4215) loss: 2.305468\n",
      "(Iteration 997 / 4215) loss: 2.296380\n",
      "(Iteration 999 / 4215) loss: 2.295786\n",
      "(Iteration 1001 / 4215) loss: 2.305825\n",
      "(Iteration 1003 / 4215) loss: 2.305926\n",
      "(Iteration 1005 / 4215) loss: 2.304493\n",
      "(Iteration 1007 / 4215) loss: 2.306216\n",
      "(Iteration 1009 / 4215) loss: 2.298895\n",
      "(Iteration 1011 / 4215) loss: 2.298422\n",
      "(Iteration 1013 / 4215) loss: 2.300844\n",
      "(Iteration 1015 / 4215) loss: 2.305812\n",
      "(Iteration 1017 / 4215) loss: 2.299204\n",
      "(Iteration 1019 / 4215) loss: 2.303967\n",
      "(Iteration 1021 / 4215) loss: 2.306386\n",
      "(Iteration 1023 / 4215) loss: 2.305259\n",
      "(Iteration 1025 / 4215) loss: 2.306958\n",
      "(Iteration 1027 / 4215) loss: 2.294850\n",
      "(Iteration 1029 / 4215) loss: 2.310928\n",
      "(Iteration 1031 / 4215) loss: 2.297288\n",
      "(Iteration 1033 / 4215) loss: 2.295131\n",
      "(Iteration 1035 / 4215) loss: 2.304098\n",
      "(Iteration 1037 / 4215) loss: 2.308495\n",
      "(Iteration 1039 / 4215) loss: 2.300900\n",
      "(Iteration 1041 / 4215) loss: 2.303505\n",
      "(Iteration 1043 / 4215) loss: 2.297813\n",
      "(Iteration 1045 / 4215) loss: 2.306151\n",
      "(Iteration 1047 / 4215) loss: 2.293069\n",
      "(Iteration 1049 / 4215) loss: 2.298617\n",
      "(Iteration 1051 / 4215) loss: 2.300658\n",
      "(Iteration 1053 / 4215) loss: 2.303085\n",
      "(Iteration 1055 / 4215) loss: 2.290762\n",
      "(Iteration 1057 / 4215) loss: 2.301939\n",
      "(Iteration 1059 / 4215) loss: 2.300484\n",
      "(Iteration 1061 / 4215) loss: 2.303872\n",
      "(Iteration 1063 / 4215) loss: 2.298493\n",
      "(Iteration 1065 / 4215) loss: 2.297678\n",
      "(Iteration 1067 / 4215) loss: 2.312868\n",
      "(Iteration 1069 / 4215) loss: 2.304658\n",
      "(Iteration 1071 / 4215) loss: 2.296727\n",
      "(Iteration 1073 / 4215) loss: 2.304372\n",
      "(Iteration 1075 / 4215) loss: 2.303624\n",
      "(Iteration 1077 / 4215) loss: 2.297033\n",
      "(Iteration 1079 / 4215) loss: 2.297474\n",
      "(Iteration 1081 / 4215) loss: 2.299919\n",
      "(Iteration 1083 / 4215) loss: 2.303002\n",
      "(Iteration 1085 / 4215) loss: 2.301072\n",
      "(Iteration 1087 / 4215) loss: 2.301825\n",
      "(Iteration 1089 / 4215) loss: 2.299994\n",
      "(Iteration 1091 / 4215) loss: 2.305598\n",
      "(Iteration 1093 / 4215) loss: 2.304823\n",
      "(Iteration 1095 / 4215) loss: 2.309242\n",
      "(Iteration 1097 / 4215) loss: 2.298904\n",
      "(Iteration 1099 / 4215) loss: 2.306336\n",
      "(Iteration 1101 / 4215) loss: 2.292908\n",
      "(Iteration 1103 / 4215) loss: 2.295751\n",
      "(Iteration 1105 / 4215) loss: 2.293935\n",
      "(Iteration 1107 / 4215) loss: 2.297127\n",
      "(Iteration 1109 / 4215) loss: 2.302338\n",
      "(Iteration 1111 / 4215) loss: 2.308235\n",
      "(Iteration 1113 / 4215) loss: 2.302606\n",
      "(Iteration 1115 / 4215) loss: 2.303213\n",
      "(Iteration 1117 / 4215) loss: 2.299748\n",
      "(Iteration 1119 / 4215) loss: 2.302789\n",
      "(Iteration 1121 / 4215) loss: 2.298670\n",
      "(Iteration 1123 / 4215) loss: 2.300607\n",
      "(Iteration 1125 / 4215) loss: 2.301277\n",
      "(Iteration 1127 / 4215) loss: 2.303250\n",
      "(Iteration 1129 / 4215) loss: 2.293712\n",
      "(Iteration 1131 / 4215) loss: 2.302503\n",
      "(Iteration 1133 / 4215) loss: 2.306664\n",
      "(Iteration 1135 / 4215) loss: 2.297462\n",
      "(Iteration 1137 / 4215) loss: 2.307286\n",
      "(Iteration 1139 / 4215) loss: 2.296076\n",
      "(Iteration 1141 / 4215) loss: 2.309505\n",
      "(Iteration 1143 / 4215) loss: 2.292545\n",
      "(Iteration 1145 / 4215) loss: 2.297365\n",
      "(Iteration 1147 / 4215) loss: 2.301189\n",
      "(Iteration 1149 / 4215) loss: 2.298457\n",
      "(Iteration 1151 / 4215) loss: 2.307277\n",
      "(Iteration 1153 / 4215) loss: 2.299902\n",
      "(Iteration 1155 / 4215) loss: 2.308133\n",
      "(Iteration 1157 / 4215) loss: 2.301788\n",
      "(Iteration 1159 / 4215) loss: 2.304801\n",
      "(Iteration 1161 / 4215) loss: 2.296240\n",
      "(Iteration 1163 / 4215) loss: 2.293955\n",
      "(Iteration 1165 / 4215) loss: 2.297369\n",
      "(Iteration 1167 / 4215) loss: 2.296395\n",
      "(Iteration 1169 / 4215) loss: 2.299122\n",
      "(Iteration 1171 / 4215) loss: 2.305075\n",
      "(Iteration 1173 / 4215) loss: 2.297423\n",
      "(Iteration 1175 / 4215) loss: 2.305405\n",
      "(Iteration 1177 / 4215) loss: 2.293342\n",
      "(Iteration 1179 / 4215) loss: 2.298694\n",
      "(Iteration 1181 / 4215) loss: 2.302911\n",
      "(Iteration 1183 / 4215) loss: 2.307984\n",
      "(Iteration 1185 / 4215) loss: 2.296454\n",
      "(Iteration 1187 / 4215) loss: 2.300884\n",
      "(Iteration 1189 / 4215) loss: 2.301458\n",
      "(Iteration 1191 / 4215) loss: 2.312377\n",
      "(Iteration 1193 / 4215) loss: 2.300889\n",
      "(Iteration 1195 / 4215) loss: 2.300232\n",
      "(Iteration 1197 / 4215) loss: 2.303898\n",
      "(Iteration 1199 / 4215) loss: 2.293034\n",
      "(Iteration 1201 / 4215) loss: 2.302795\n",
      "(Iteration 1203 / 4215) loss: 2.294478\n",
      "(Iteration 1205 / 4215) loss: 2.309797\n",
      "(Iteration 1207 / 4215) loss: 2.299206\n",
      "(Iteration 1209 / 4215) loss: 2.308018\n",
      "(Iteration 1211 / 4215) loss: 2.306461\n",
      "(Iteration 1213 / 4215) loss: 2.296908\n",
      "(Iteration 1215 / 4215) loss: 2.310040\n",
      "(Iteration 1217 / 4215) loss: 2.308024\n",
      "(Iteration 1219 / 4215) loss: 2.306092\n",
      "(Iteration 1221 / 4215) loss: 2.299826\n",
      "(Iteration 1223 / 4215) loss: 2.314447\n",
      "(Iteration 1225 / 4215) loss: 2.303435\n",
      "(Iteration 1227 / 4215) loss: 2.302703\n",
      "(Iteration 1229 / 4215) loss: 2.306462\n",
      "(Iteration 1231 / 4215) loss: 2.298492\n",
      "(Iteration 1233 / 4215) loss: 2.299364\n",
      "(Iteration 1235 / 4215) loss: 2.303700\n",
      "(Iteration 1237 / 4215) loss: 2.302807\n",
      "(Iteration 1239 / 4215) loss: 2.308745\n",
      "(Iteration 1241 / 4215) loss: 2.292382\n",
      "(Iteration 1243 / 4215) loss: 2.297805\n",
      "(Iteration 1245 / 4215) loss: 2.304480\n",
      "(Iteration 1247 / 4215) loss: 2.307865\n",
      "(Iteration 1249 / 4215) loss: 2.308402\n",
      "(Iteration 1251 / 4215) loss: 2.297841\n",
      "(Iteration 1253 / 4215) loss: 2.296150\n",
      "(Iteration 1255 / 4215) loss: 2.301545\n",
      "(Iteration 1257 / 4215) loss: 2.299836\n",
      "(Iteration 1259 / 4215) loss: 2.294964\n",
      "(Iteration 1261 / 4215) loss: 2.299462\n",
      "(Iteration 1263 / 4215) loss: 2.302245\n",
      "(Iteration 1265 / 4215) loss: 2.308514\n",
      "(Iteration 1267 / 4215) loss: 2.303662\n",
      "(Iteration 1269 / 4215) loss: 2.301035\n",
      "(Iteration 1271 / 4215) loss: 2.293385\n",
      "(Iteration 1273 / 4215) loss: 2.296220\n",
      "(Iteration 1275 / 4215) loss: 2.299862\n",
      "(Iteration 1277 / 4215) loss: 2.310823\n",
      "(Iteration 1279 / 4215) loss: 2.296617\n",
      "(Iteration 1281 / 4215) loss: 2.303194\n",
      "(Iteration 1283 / 4215) loss: 2.309655\n",
      "(Iteration 1285 / 4215) loss: 2.309293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1287 / 4215) loss: 2.304322\n",
      "(Iteration 1289 / 4215) loss: 2.298238\n",
      "(Iteration 1291 / 4215) loss: 2.296111\n",
      "(Iteration 1293 / 4215) loss: 2.298188\n",
      "(Iteration 1295 / 4215) loss: 2.298322\n",
      "(Iteration 1297 / 4215) loss: 2.295434\n",
      "(Iteration 1299 / 4215) loss: 2.295151\n",
      "(Iteration 1301 / 4215) loss: 2.297286\n",
      "(Iteration 1303 / 4215) loss: 2.298478\n",
      "(Iteration 1305 / 4215) loss: 2.302198\n",
      "(Iteration 1307 / 4215) loss: 2.298498\n",
      "(Iteration 1309 / 4215) loss: 2.297820\n",
      "(Iteration 1311 / 4215) loss: 2.301275\n",
      "(Iteration 1313 / 4215) loss: 2.293647\n",
      "(Iteration 1315 / 4215) loss: 2.306398\n",
      "(Iteration 1317 / 4215) loss: 2.302052\n",
      "(Iteration 1319 / 4215) loss: 2.294975\n",
      "(Iteration 1321 / 4215) loss: 2.307694\n",
      "(Iteration 1323 / 4215) loss: 2.307773\n",
      "(Iteration 1325 / 4215) loss: 2.292208\n",
      "(Iteration 1327 / 4215) loss: 2.296176\n",
      "(Iteration 1329 / 4215) loss: 2.304150\n",
      "(Iteration 1331 / 4215) loss: 2.302517\n",
      "(Iteration 1333 / 4215) loss: 2.305951\n",
      "(Iteration 1335 / 4215) loss: 2.301858\n",
      "(Iteration 1337 / 4215) loss: 2.300288\n",
      "(Iteration 1339 / 4215) loss: 2.310434\n",
      "(Iteration 1341 / 4215) loss: 2.298945\n",
      "(Iteration 1343 / 4215) loss: 2.299285\n",
      "(Iteration 1345 / 4215) loss: 2.303124\n",
      "(Iteration 1347 / 4215) loss: 2.306255\n",
      "(Iteration 1349 / 4215) loss: 2.300089\n",
      "(Iteration 1351 / 4215) loss: 2.293228\n",
      "(Iteration 1353 / 4215) loss: 2.300078\n",
      "(Iteration 1355 / 4215) loss: 2.295751\n",
      "(Iteration 1357 / 4215) loss: 2.300915\n",
      "(Iteration 1359 / 4215) loss: 2.305498\n",
      "(Iteration 1361 / 4215) loss: 2.308483\n",
      "(Iteration 1363 / 4215) loss: 2.308991\n",
      "(Iteration 1365 / 4215) loss: 2.305403\n",
      "(Iteration 1367 / 4215) loss: 2.292765\n",
      "(Iteration 1369 / 4215) loss: 2.308812\n",
      "(Iteration 1371 / 4215) loss: 2.305116\n",
      "(Iteration 1373 / 4215) loss: 2.302066\n",
      "(Iteration 1375 / 4215) loss: 2.291928\n",
      "(Iteration 1377 / 4215) loss: 2.304581\n",
      "(Iteration 1379 / 4215) loss: 2.290724\n",
      "(Iteration 1381 / 4215) loss: 2.294436\n",
      "(Iteration 1383 / 4215) loss: 2.302896\n",
      "(Iteration 1385 / 4215) loss: 2.304563\n",
      "(Iteration 1387 / 4215) loss: 2.303361\n",
      "(Iteration 1389 / 4215) loss: 2.297057\n",
      "(Iteration 1391 / 4215) loss: 2.301478\n",
      "(Iteration 1393 / 4215) loss: 2.299181\n",
      "(Iteration 1395 / 4215) loss: 2.306242\n",
      "(Iteration 1397 / 4215) loss: 2.295910\n",
      "(Iteration 1399 / 4215) loss: 2.298589\n",
      "(Iteration 1401 / 4215) loss: 2.302386\n",
      "(Iteration 1403 / 4215) loss: 2.303134\n",
      "(Iteration 1405 / 4215) loss: 2.295618\n",
      "(Iteration 1407 / 4215) loss: 2.296643\n",
      "(Iteration 1409 / 4215) loss: 2.281949\n",
      "(Iteration 1411 / 4215) loss: 2.301452\n",
      "(Iteration 1413 / 4215) loss: 2.296541\n",
      "(Iteration 1415 / 4215) loss: 2.309159\n",
      "(Iteration 1417 / 4215) loss: 2.294462\n",
      "(Iteration 1419 / 4215) loss: 2.305687\n",
      "(Iteration 1421 / 4215) loss: 2.300330\n",
      "(Iteration 1423 / 4215) loss: 2.306394\n",
      "(Iteration 1425 / 4215) loss: 2.304582\n",
      "(Iteration 1427 / 4215) loss: 2.302752\n",
      "(Iteration 1429 / 4215) loss: 2.300999\n",
      "(Iteration 1431 / 4215) loss: 2.293854\n",
      "(Iteration 1433 / 4215) loss: 2.298141\n",
      "(Iteration 1435 / 4215) loss: 2.306935\n",
      "(Iteration 1437 / 4215) loss: 2.297149\n",
      "(Iteration 1439 / 4215) loss: 2.308101\n",
      "(Iteration 1441 / 4215) loss: 2.303500\n",
      "(Iteration 1443 / 4215) loss: 2.295677\n",
      "(Iteration 1445 / 4215) loss: 2.299501\n",
      "(Iteration 1447 / 4215) loss: 2.303103\n",
      "(Iteration 1449 / 4215) loss: 2.303394\n",
      "(Iteration 1451 / 4215) loss: 2.305041\n",
      "(Iteration 1453 / 4215) loss: 2.298359\n",
      "(Iteration 1455 / 4215) loss: 2.298890\n",
      "(Iteration 1457 / 4215) loss: 2.296911\n",
      "(Iteration 1459 / 4215) loss: 2.295612\n",
      "(Iteration 1461 / 4215) loss: 2.299529\n",
      "(Iteration 1463 / 4215) loss: 2.302443\n",
      "(Iteration 1465 / 4215) loss: 2.308187\n",
      "(Iteration 1467 / 4215) loss: 2.299712\n",
      "(Iteration 1469 / 4215) loss: 2.306727\n",
      "(Iteration 1471 / 4215) loss: 2.302351\n",
      "(Iteration 1473 / 4215) loss: 2.299805\n",
      "(Iteration 1475 / 4215) loss: 2.311678\n",
      "(Iteration 1477 / 4215) loss: 2.299114\n",
      "(Iteration 1479 / 4215) loss: 2.308116\n",
      "(Iteration 1481 / 4215) loss: 2.297098\n",
      "(Iteration 1483 / 4215) loss: 2.300259\n",
      "(Iteration 1485 / 4215) loss: 2.307088\n",
      "(Iteration 1487 / 4215) loss: 2.298685\n",
      "(Iteration 1489 / 4215) loss: 2.301095\n",
      "(Iteration 1491 / 4215) loss: 2.302173\n",
      "(Iteration 1493 / 4215) loss: 2.302109\n",
      "(Iteration 1495 / 4215) loss: 2.300308\n",
      "(Iteration 1497 / 4215) loss: 2.297761\n",
      "(Iteration 1499 / 4215) loss: 2.293318\n",
      "(Iteration 1501 / 4215) loss: 2.299472\n",
      "(Iteration 1503 / 4215) loss: 2.302068\n",
      "(Iteration 1505 / 4215) loss: 2.302576\n",
      "(Iteration 1507 / 4215) loss: 2.302683\n",
      "(Iteration 1509 / 4215) loss: 2.304442\n",
      "(Iteration 1511 / 4215) loss: 2.302888\n",
      "(Iteration 1513 / 4215) loss: 2.302981\n",
      "(Iteration 1515 / 4215) loss: 2.305887\n",
      "(Iteration 1517 / 4215) loss: 2.293876\n",
      "(Iteration 1519 / 4215) loss: 2.308662\n",
      "(Iteration 1521 / 4215) loss: 2.301015\n",
      "(Iteration 1523 / 4215) loss: 2.298371\n",
      "(Iteration 1525 / 4215) loss: 2.302433\n",
      "(Iteration 1527 / 4215) loss: 2.296005\n",
      "(Iteration 1529 / 4215) loss: 2.301936\n",
      "(Iteration 1531 / 4215) loss: 2.300397\n",
      "(Iteration 1533 / 4215) loss: 2.301870\n",
      "(Iteration 1535 / 4215) loss: 2.302661\n",
      "(Iteration 1537 / 4215) loss: 2.304239\n",
      "(Iteration 1539 / 4215) loss: 2.300973\n",
      "(Iteration 1541 / 4215) loss: 2.302251\n",
      "(Iteration 1543 / 4215) loss: 2.303378\n",
      "(Iteration 1545 / 4215) loss: 2.303363\n",
      "(Iteration 1547 / 4215) loss: 2.305792\n",
      "(Iteration 1549 / 4215) loss: 2.296918\n",
      "(Iteration 1551 / 4215) loss: 2.303803\n",
      "(Iteration 1553 / 4215) loss: 2.289685\n",
      "(Iteration 1555 / 4215) loss: 2.301772\n",
      "(Iteration 1557 / 4215) loss: 2.302754\n",
      "(Iteration 1559 / 4215) loss: 2.303852\n",
      "(Iteration 1561 / 4215) loss: 2.308567\n",
      "(Iteration 1563 / 4215) loss: 2.307027\n",
      "(Iteration 1565 / 4215) loss: 2.303383\n",
      "(Iteration 1567 / 4215) loss: 2.304997\n",
      "(Iteration 1569 / 4215) loss: 2.315530\n",
      "(Iteration 1571 / 4215) loss: 2.308048\n",
      "(Iteration 1573 / 4215) loss: 2.299660\n",
      "(Iteration 1575 / 4215) loss: 2.298658\n",
      "(Iteration 1577 / 4215) loss: 2.300374\n",
      "(Iteration 1579 / 4215) loss: 2.313047\n",
      "(Iteration 1581 / 4215) loss: 2.303176\n",
      "(Iteration 1583 / 4215) loss: 2.300547\n",
      "(Iteration 1585 / 4215) loss: 2.298677\n",
      "(Iteration 1587 / 4215) loss: 2.300166\n",
      "(Iteration 1589 / 4215) loss: 2.304116\n",
      "(Iteration 1591 / 4215) loss: 2.304562\n",
      "(Iteration 1593 / 4215) loss: 2.301344\n",
      "(Iteration 1595 / 4215) loss: 2.303462\n",
      "(Iteration 1597 / 4215) loss: 2.305363\n",
      "(Iteration 1599 / 4215) loss: 2.297076\n",
      "(Iteration 1601 / 4215) loss: 2.293620\n",
      "(Iteration 1603 / 4215) loss: 2.301853\n",
      "(Iteration 1605 / 4215) loss: 2.291212\n",
      "(Iteration 1607 / 4215) loss: 2.309653\n",
      "(Iteration 1609 / 4215) loss: 2.293131\n",
      "(Iteration 1611 / 4215) loss: 2.305453\n",
      "(Iteration 1613 / 4215) loss: 2.303081\n",
      "(Iteration 1615 / 4215) loss: 2.295118\n",
      "(Iteration 1617 / 4215) loss: 2.300370\n",
      "(Iteration 1619 / 4215) loss: 2.300300\n",
      "(Iteration 1621 / 4215) loss: 2.294453\n",
      "(Iteration 1623 / 4215) loss: 2.301549\n",
      "(Iteration 1625 / 4215) loss: 2.301072\n",
      "(Iteration 1627 / 4215) loss: 2.301399\n",
      "(Iteration 1629 / 4215) loss: 2.306443\n",
      "(Iteration 1631 / 4215) loss: 2.292106\n",
      "(Iteration 1633 / 4215) loss: 2.296567\n",
      "(Iteration 1635 / 4215) loss: 2.302374\n",
      "(Iteration 1637 / 4215) loss: 2.296150\n",
      "(Iteration 1639 / 4215) loss: 2.307013\n",
      "(Iteration 1641 / 4215) loss: 2.306081\n",
      "(Iteration 1643 / 4215) loss: 2.299870\n",
      "(Iteration 1645 / 4215) loss: 2.299229\n",
      "(Iteration 1647 / 4215) loss: 2.300936\n",
      "(Iteration 1649 / 4215) loss: 2.301554\n",
      "(Iteration 1651 / 4215) loss: 2.297647\n",
      "(Iteration 1653 / 4215) loss: 2.310904\n",
      "(Iteration 1655 / 4215) loss: 2.303883\n",
      "(Iteration 1657 / 4215) loss: 2.308478\n",
      "(Iteration 1659 / 4215) loss: 2.295922\n",
      "(Iteration 1661 / 4215) loss: 2.300411\n",
      "(Iteration 1663 / 4215) loss: 2.303494\n",
      "(Iteration 1665 / 4215) loss: 2.309699\n",
      "(Iteration 1667 / 4215) loss: 2.300992\n",
      "(Iteration 1669 / 4215) loss: 2.298011\n",
      "(Iteration 1671 / 4215) loss: 2.296904\n",
      "(Iteration 1673 / 4215) loss: 2.308422\n",
      "(Iteration 1675 / 4215) loss: 2.302604\n",
      "(Iteration 1677 / 4215) loss: 2.304770\n",
      "(Iteration 1679 / 4215) loss: 2.295395\n",
      "(Iteration 1681 / 4215) loss: 2.292006\n",
      "(Iteration 1683 / 4215) loss: 2.288910\n",
      "(Iteration 1685 / 4215) loss: 2.299297\n",
      "(Epoch 2 / 5) train acc: 0.112000; val_acc: 0.115000\n",
      "(Iteration 1687 / 4215) loss: 2.307890\n",
      "(Iteration 1689 / 4215) loss: 2.297396\n",
      "(Iteration 1691 / 4215) loss: 2.298614\n",
      "(Iteration 1693 / 4215) loss: 2.303482\n",
      "(Iteration 1695 / 4215) loss: 2.293974\n",
      "(Iteration 1697 / 4215) loss: 2.305166\n",
      "(Iteration 1699 / 4215) loss: 2.309887\n",
      "(Iteration 1701 / 4215) loss: 2.303618\n",
      "(Iteration 1703 / 4215) loss: 2.298866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1705 / 4215) loss: 2.300733\n",
      "(Iteration 1707 / 4215) loss: 2.302930\n",
      "(Iteration 1709 / 4215) loss: 2.306828\n",
      "(Iteration 1711 / 4215) loss: 2.306148\n",
      "(Iteration 1713 / 4215) loss: 2.310915\n",
      "(Iteration 1715 / 4215) loss: 2.303696\n",
      "(Iteration 1717 / 4215) loss: 2.298620\n",
      "(Iteration 1719 / 4215) loss: 2.292306\n",
      "(Iteration 1721 / 4215) loss: 2.307254\n",
      "(Iteration 1723 / 4215) loss: 2.297098\n",
      "(Iteration 1725 / 4215) loss: 2.302403\n",
      "(Iteration 1727 / 4215) loss: 2.300067\n",
      "(Iteration 1729 / 4215) loss: 2.306825\n",
      "(Iteration 1731 / 4215) loss: 2.301333\n",
      "(Iteration 1733 / 4215) loss: 2.299125\n",
      "(Iteration 1735 / 4215) loss: 2.301309\n",
      "(Iteration 1737 / 4215) loss: 2.294913\n",
      "(Iteration 1739 / 4215) loss: 2.306857\n",
      "(Iteration 1741 / 4215) loss: 2.303810\n",
      "(Iteration 1743 / 4215) loss: 2.298886\n",
      "(Iteration 1745 / 4215) loss: 2.295679\n",
      "(Iteration 1747 / 4215) loss: 2.297325\n",
      "(Iteration 1749 / 4215) loss: 2.300447\n",
      "(Iteration 1751 / 4215) loss: 2.304801\n",
      "(Iteration 1753 / 4215) loss: 2.303922\n",
      "(Iteration 1755 / 4215) loss: 2.303758\n",
      "(Iteration 1757 / 4215) loss: 2.304186\n",
      "(Iteration 1759 / 4215) loss: 2.301023\n",
      "(Iteration 1761 / 4215) loss: 2.297121\n",
      "(Iteration 1763 / 4215) loss: 2.308050\n",
      "(Iteration 1765 / 4215) loss: 2.300806\n",
      "(Iteration 1767 / 4215) loss: 2.309472\n",
      "(Iteration 1769 / 4215) loss: 2.298835\n",
      "(Iteration 1771 / 4215) loss: 2.302429\n",
      "(Iteration 1773 / 4215) loss: 2.292020\n",
      "(Iteration 1775 / 4215) loss: 2.304090\n",
      "(Iteration 1777 / 4215) loss: 2.299221\n",
      "(Iteration 1779 / 4215) loss: 2.300641\n",
      "(Iteration 1781 / 4215) loss: 2.300548\n",
      "(Iteration 1783 / 4215) loss: 2.302887\n",
      "(Iteration 1785 / 4215) loss: 2.303294\n",
      "(Iteration 1787 / 4215) loss: 2.290606\n",
      "(Iteration 1789 / 4215) loss: 2.300627\n",
      "(Iteration 1791 / 4215) loss: 2.308501\n",
      "(Iteration 1793 / 4215) loss: 2.305417\n",
      "(Iteration 1795 / 4215) loss: 2.306618\n",
      "(Iteration 1797 / 4215) loss: 2.308807\n",
      "(Iteration 1799 / 4215) loss: 2.309945\n",
      "(Iteration 1801 / 4215) loss: 2.303518\n",
      "(Iteration 1803 / 4215) loss: 2.299088\n",
      "(Iteration 1805 / 4215) loss: 2.288283\n",
      "(Iteration 1807 / 4215) loss: 2.300213\n",
      "(Iteration 1809 / 4215) loss: 2.306234\n",
      "(Iteration 1811 / 4215) loss: 2.294268\n",
      "(Iteration 1813 / 4215) loss: 2.303987\n",
      "(Iteration 1815 / 4215) loss: 2.295022\n",
      "(Iteration 1817 / 4215) loss: 2.288314\n",
      "(Iteration 1819 / 4215) loss: 2.300898\n",
      "(Iteration 1821 / 4215) loss: 2.301871\n",
      "(Iteration 1823 / 4215) loss: 2.308016\n",
      "(Iteration 1825 / 4215) loss: 2.298862\n",
      "(Iteration 1827 / 4215) loss: 2.306728\n",
      "(Iteration 1829 / 4215) loss: 2.299507\n",
      "(Iteration 1831 / 4215) loss: 2.303304\n",
      "(Iteration 1833 / 4215) loss: 2.298463\n",
      "(Iteration 1835 / 4215) loss: 2.308145\n",
      "(Iteration 1837 / 4215) loss: 2.294320\n",
      "(Iteration 1839 / 4215) loss: 2.306148\n",
      "(Iteration 1841 / 4215) loss: 2.289785\n",
      "(Iteration 1843 / 4215) loss: 2.298458\n",
      "(Iteration 1845 / 4215) loss: 2.300321\n",
      "(Iteration 1847 / 4215) loss: 2.303826\n",
      "(Iteration 1849 / 4215) loss: 2.292035\n",
      "(Iteration 1851 / 4215) loss: 2.310373\n",
      "(Iteration 1853 / 4215) loss: 2.307186\n",
      "(Iteration 1855 / 4215) loss: 2.306919\n",
      "(Iteration 1857 / 4215) loss: 2.311942\n",
      "(Iteration 1859 / 4215) loss: 2.298832\n",
      "(Iteration 1861 / 4215) loss: 2.304620\n",
      "(Iteration 1863 / 4215) loss: 2.304397\n",
      "(Iteration 1865 / 4215) loss: 2.297603\n",
      "(Iteration 1867 / 4215) loss: 2.302920\n",
      "(Iteration 1869 / 4215) loss: 2.307785\n",
      "(Iteration 1871 / 4215) loss: 2.304670\n",
      "(Iteration 1873 / 4215) loss: 2.304171\n",
      "(Iteration 1875 / 4215) loss: 2.306969\n",
      "(Iteration 1877 / 4215) loss: 2.295912\n",
      "(Iteration 1879 / 4215) loss: 2.305156\n",
      "(Iteration 1881 / 4215) loss: 2.297921\n",
      "(Iteration 1883 / 4215) loss: 2.295849\n",
      "(Iteration 1885 / 4215) loss: 2.300593\n",
      "(Iteration 1887 / 4215) loss: 2.291766\n",
      "(Iteration 1889 / 4215) loss: 2.296195\n",
      "(Iteration 1891 / 4215) loss: 2.297409\n",
      "(Iteration 1893 / 4215) loss: 2.299581\n",
      "(Iteration 1895 / 4215) loss: 2.302245\n",
      "(Iteration 1897 / 4215) loss: 2.298858\n",
      "(Iteration 1899 / 4215) loss: 2.301855\n",
      "(Iteration 1901 / 4215) loss: 2.294654\n",
      "(Iteration 1903 / 4215) loss: 2.303708\n",
      "(Iteration 1905 / 4215) loss: 2.294469\n",
      "(Iteration 1907 / 4215) loss: 2.302605\n",
      "(Iteration 1909 / 4215) loss: 2.298415\n",
      "(Iteration 1911 / 4215) loss: 2.302487\n",
      "(Iteration 1913 / 4215) loss: 2.302551\n",
      "(Iteration 1915 / 4215) loss: 2.301221\n",
      "(Iteration 1917 / 4215) loss: 2.295720\n",
      "(Iteration 1919 / 4215) loss: 2.304987\n",
      "(Iteration 1921 / 4215) loss: 2.304809\n",
      "(Iteration 1923 / 4215) loss: 2.295255\n",
      "(Iteration 1925 / 4215) loss: 2.293002\n",
      "(Iteration 1927 / 4215) loss: 2.303346\n",
      "(Iteration 1929 / 4215) loss: 2.308754\n",
      "(Iteration 1931 / 4215) loss: 2.298943\n",
      "(Iteration 1933 / 4215) loss: 2.295020\n",
      "(Iteration 1935 / 4215) loss: 2.298234\n",
      "(Iteration 1937 / 4215) loss: 2.303461\n",
      "(Iteration 1939 / 4215) loss: 2.300803\n",
      "(Iteration 1941 / 4215) loss: 2.300354\n",
      "(Iteration 1943 / 4215) loss: 2.289806\n",
      "(Iteration 1945 / 4215) loss: 2.302867\n",
      "(Iteration 1947 / 4215) loss: 2.303511\n",
      "(Iteration 1949 / 4215) loss: 2.300989\n",
      "(Iteration 1951 / 4215) loss: 2.299193\n",
      "(Iteration 1953 / 4215) loss: 2.289935\n",
      "(Iteration 1955 / 4215) loss: 2.311507\n",
      "(Iteration 1957 / 4215) loss: 2.299415\n",
      "(Iteration 1959 / 4215) loss: 2.301294\n",
      "(Iteration 1961 / 4215) loss: 2.303984\n",
      "(Iteration 1963 / 4215) loss: 2.303531\n",
      "(Iteration 1965 / 4215) loss: 2.297563\n",
      "(Iteration 1967 / 4215) loss: 2.298504\n",
      "(Iteration 1969 / 4215) loss: 2.296665\n",
      "(Iteration 1971 / 4215) loss: 2.296637\n",
      "(Iteration 1973 / 4215) loss: 2.298297\n",
      "(Iteration 1975 / 4215) loss: 2.305238\n",
      "(Iteration 1977 / 4215) loss: 2.307999\n",
      "(Iteration 1979 / 4215) loss: 2.289960\n",
      "(Iteration 1981 / 4215) loss: 2.294660\n",
      "(Iteration 1983 / 4215) loss: 2.299898\n",
      "(Iteration 1985 / 4215) loss: 2.301986\n",
      "(Iteration 1987 / 4215) loss: 2.296316\n",
      "(Iteration 1989 / 4215) loss: 2.308977\n",
      "(Iteration 1991 / 4215) loss: 2.305864\n",
      "(Iteration 1993 / 4215) loss: 2.314327\n",
      "(Iteration 1995 / 4215) loss: 2.297785\n",
      "(Iteration 1997 / 4215) loss: 2.296134\n",
      "(Iteration 1999 / 4215) loss: 2.309234\n",
      "(Iteration 2001 / 4215) loss: 2.300591\n",
      "(Iteration 2003 / 4215) loss: 2.301956\n",
      "(Iteration 2005 / 4215) loss: 2.305086\n",
      "(Iteration 2007 / 4215) loss: 2.308096\n",
      "(Iteration 2009 / 4215) loss: 2.303336\n",
      "(Iteration 2011 / 4215) loss: 2.300164\n",
      "(Iteration 2013 / 4215) loss: 2.302332\n",
      "(Iteration 2015 / 4215) loss: 2.299055\n",
      "(Iteration 2017 / 4215) loss: 2.297242\n",
      "(Iteration 2019 / 4215) loss: 2.302686\n",
      "(Iteration 2021 / 4215) loss: 2.285922\n",
      "(Iteration 2023 / 4215) loss: 2.299087\n",
      "(Iteration 2025 / 4215) loss: 2.297363\n",
      "(Iteration 2027 / 4215) loss: 2.304162\n",
      "(Iteration 2029 / 4215) loss: 2.296758\n",
      "(Iteration 2031 / 4215) loss: 2.302787\n",
      "(Iteration 2033 / 4215) loss: 2.307218\n",
      "(Iteration 2035 / 4215) loss: 2.314566\n",
      "(Iteration 2037 / 4215) loss: 2.300657\n",
      "(Iteration 2039 / 4215) loss: 2.299169\n",
      "(Iteration 2041 / 4215) loss: 2.301919\n",
      "(Iteration 2043 / 4215) loss: 2.309217\n",
      "(Iteration 2045 / 4215) loss: 2.301047\n",
      "(Iteration 2047 / 4215) loss: 2.299523\n",
      "(Iteration 2049 / 4215) loss: 2.299409\n",
      "(Iteration 2051 / 4215) loss: 2.299008\n",
      "(Iteration 2053 / 4215) loss: 2.303205\n",
      "(Iteration 2055 / 4215) loss: 2.306922\n",
      "(Iteration 2057 / 4215) loss: 2.304518\n",
      "(Iteration 2059 / 4215) loss: 2.308678\n",
      "(Iteration 2061 / 4215) loss: 2.298583\n",
      "(Iteration 2063 / 4215) loss: 2.300251\n",
      "(Iteration 2065 / 4215) loss: 2.300941\n",
      "(Iteration 2067 / 4215) loss: 2.304113\n",
      "(Iteration 2069 / 4215) loss: 2.299579\n",
      "(Iteration 2071 / 4215) loss: 2.299288\n",
      "(Iteration 2073 / 4215) loss: 2.302250\n",
      "(Iteration 2075 / 4215) loss: 2.307168\n",
      "(Iteration 2077 / 4215) loss: 2.300332\n",
      "(Iteration 2079 / 4215) loss: 2.305610\n",
      "(Iteration 2081 / 4215) loss: 2.302090\n",
      "(Iteration 2083 / 4215) loss: 2.308707\n",
      "(Iteration 2085 / 4215) loss: 2.298046\n",
      "(Iteration 2087 / 4215) loss: 2.304716\n",
      "(Iteration 2089 / 4215) loss: 2.298718\n",
      "(Iteration 2091 / 4215) loss: 2.306796\n",
      "(Iteration 2093 / 4215) loss: 2.305863\n",
      "(Iteration 2095 / 4215) loss: 2.306991\n",
      "(Iteration 2097 / 4215) loss: 2.304393\n",
      "(Iteration 2099 / 4215) loss: 2.297572\n",
      "(Iteration 2101 / 4215) loss: 2.300622\n",
      "(Iteration 2103 / 4215) loss: 2.297199\n",
      "(Iteration 2105 / 4215) loss: 2.303644\n",
      "(Iteration 2107 / 4215) loss: 2.297026\n",
      "(Iteration 2109 / 4215) loss: 2.299957\n",
      "(Iteration 2111 / 4215) loss: 2.297901\n",
      "(Iteration 2113 / 4215) loss: 2.305477\n",
      "(Iteration 2115 / 4215) loss: 2.298339\n",
      "(Iteration 2117 / 4215) loss: 2.293104\n",
      "(Iteration 2119 / 4215) loss: 2.296300\n",
      "(Iteration 2121 / 4215) loss: 2.296990\n",
      "(Iteration 2123 / 4215) loss: 2.299628\n",
      "(Iteration 2125 / 4215) loss: 2.295148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 2127 / 4215) loss: 2.299195\n",
      "(Iteration 2129 / 4215) loss: 2.295693\n",
      "(Iteration 2131 / 4215) loss: 2.293676\n",
      "(Iteration 2133 / 4215) loss: 2.300928\n",
      "(Iteration 2135 / 4215) loss: 2.298678\n",
      "(Iteration 2137 / 4215) loss: 2.311382\n",
      "(Iteration 2139 / 4215) loss: 2.294124\n",
      "(Iteration 2141 / 4215) loss: 2.295352\n",
      "(Iteration 2143 / 4215) loss: 2.302654\n",
      "(Iteration 2145 / 4215) loss: 2.303350\n",
      "(Iteration 2147 / 4215) loss: 2.297709\n",
      "(Iteration 2149 / 4215) loss: 2.304446\n",
      "(Iteration 2151 / 4215) loss: 2.309658\n",
      "(Iteration 2153 / 4215) loss: 2.293216\n",
      "(Iteration 2155 / 4215) loss: 2.301749\n",
      "(Iteration 2157 / 4215) loss: 2.292084\n",
      "(Iteration 2159 / 4215) loss: 2.290095\n",
      "(Iteration 2161 / 4215) loss: 2.300952\n",
      "(Iteration 2163 / 4215) loss: 2.307547\n",
      "(Iteration 2165 / 4215) loss: 2.301376\n",
      "(Iteration 2167 / 4215) loss: 2.302676\n",
      "(Iteration 2169 / 4215) loss: 2.303822\n",
      "(Iteration 2171 / 4215) loss: 2.292131\n",
      "(Iteration 2173 / 4215) loss: 2.304501\n",
      "(Iteration 2175 / 4215) loss: 2.301822\n",
      "(Iteration 2177 / 4215) loss: 2.308818\n",
      "(Iteration 2179 / 4215) loss: 2.301587\n",
      "(Iteration 2181 / 4215) loss: 2.305744\n",
      "(Iteration 2183 / 4215) loss: 2.298887\n",
      "(Iteration 2185 / 4215) loss: 2.294843\n",
      "(Iteration 2187 / 4215) loss: 2.316026\n",
      "(Iteration 2189 / 4215) loss: 2.301078\n",
      "(Iteration 2191 / 4215) loss: 2.298604\n",
      "(Iteration 2193 / 4215) loss: 2.290698\n",
      "(Iteration 2195 / 4215) loss: 2.295794\n",
      "(Iteration 2197 / 4215) loss: 2.298074\n",
      "(Iteration 2199 / 4215) loss: 2.303703\n",
      "(Iteration 2201 / 4215) loss: 2.300659\n",
      "(Iteration 2203 / 4215) loss: 2.304511\n",
      "(Iteration 2205 / 4215) loss: 2.296617\n",
      "(Iteration 2207 / 4215) loss: 2.306813\n",
      "(Iteration 2209 / 4215) loss: 2.304919\n",
      "(Iteration 2211 / 4215) loss: 2.301386\n",
      "(Iteration 2213 / 4215) loss: 2.302999\n",
      "(Iteration 2215 / 4215) loss: 2.303427\n",
      "(Iteration 2217 / 4215) loss: 2.294179\n",
      "(Iteration 2219 / 4215) loss: 2.302207\n",
      "(Iteration 2221 / 4215) loss: 2.306849\n",
      "(Iteration 2223 / 4215) loss: 2.289050\n",
      "(Iteration 2225 / 4215) loss: 2.301376\n",
      "(Iteration 2227 / 4215) loss: 2.301727\n",
      "(Iteration 2229 / 4215) loss: 2.300077\n",
      "(Iteration 2231 / 4215) loss: 2.305679\n",
      "(Iteration 2233 / 4215) loss: 2.296901\n",
      "(Iteration 2235 / 4215) loss: 2.304166\n",
      "(Iteration 2237 / 4215) loss: 2.302312\n",
      "(Iteration 2239 / 4215) loss: 2.304217\n",
      "(Iteration 2241 / 4215) loss: 2.298702\n",
      "(Iteration 2243 / 4215) loss: 2.293146\n",
      "(Iteration 2245 / 4215) loss: 2.301242\n",
      "(Iteration 2247 / 4215) loss: 2.305157\n",
      "(Iteration 2249 / 4215) loss: 2.300899\n",
      "(Iteration 2251 / 4215) loss: 2.303685\n",
      "(Iteration 2253 / 4215) loss: 2.293074\n",
      "(Iteration 2255 / 4215) loss: 2.311760\n",
      "(Iteration 2257 / 4215) loss: 2.304834\n",
      "(Iteration 2259 / 4215) loss: 2.299745\n",
      "(Iteration 2261 / 4215) loss: 2.293515\n",
      "(Iteration 2263 / 4215) loss: 2.304144\n",
      "(Iteration 2265 / 4215) loss: 2.292986\n",
      "(Iteration 2267 / 4215) loss: 2.292582\n",
      "(Iteration 2269 / 4215) loss: 2.303246\n",
      "(Iteration 2271 / 4215) loss: 2.305367\n",
      "(Iteration 2273 / 4215) loss: 2.304482\n",
      "(Iteration 2275 / 4215) loss: 2.289982\n",
      "(Iteration 2277 / 4215) loss: 2.298746\n",
      "(Iteration 2279 / 4215) loss: 2.303637\n",
      "(Iteration 2281 / 4215) loss: 2.293135\n",
      "(Iteration 2283 / 4215) loss: 2.304581\n",
      "(Iteration 2285 / 4215) loss: 2.302993\n",
      "(Iteration 2287 / 4215) loss: 2.285740\n",
      "(Iteration 2289 / 4215) loss: 2.304934\n",
      "(Iteration 2291 / 4215) loss: 2.305190\n",
      "(Iteration 2293 / 4215) loss: 2.299053\n",
      "(Iteration 2295 / 4215) loss: 2.297234\n",
      "(Iteration 2297 / 4215) loss: 2.304180\n",
      "(Iteration 2299 / 4215) loss: 2.311702\n",
      "(Iteration 2301 / 4215) loss: 2.290664\n",
      "(Iteration 2303 / 4215) loss: 2.301993\n",
      "(Iteration 2305 / 4215) loss: 2.308280\n",
      "(Iteration 2307 / 4215) loss: 2.310397\n",
      "(Iteration 2309 / 4215) loss: 2.289597\n",
      "(Iteration 2311 / 4215) loss: 2.309438\n",
      "(Iteration 2313 / 4215) loss: 2.309701\n",
      "(Iteration 2315 / 4215) loss: 2.296328\n",
      "(Iteration 2317 / 4215) loss: 2.305503\n",
      "(Iteration 2319 / 4215) loss: 2.302324\n",
      "(Iteration 2321 / 4215) loss: 2.312103\n",
      "(Iteration 2323 / 4215) loss: 2.289273\n",
      "(Iteration 2325 / 4215) loss: 2.300994\n",
      "(Iteration 2327 / 4215) loss: 2.302228\n",
      "(Iteration 2329 / 4215) loss: 2.303997\n",
      "(Iteration 2331 / 4215) loss: 2.296197\n",
      "(Iteration 2333 / 4215) loss: 2.306262\n",
      "(Iteration 2335 / 4215) loss: 2.304690\n",
      "(Iteration 2337 / 4215) loss: 2.303604\n",
      "(Iteration 2339 / 4215) loss: 2.304952\n",
      "(Iteration 2341 / 4215) loss: 2.305402\n",
      "(Iteration 2343 / 4215) loss: 2.302570\n",
      "(Iteration 2345 / 4215) loss: 2.294951\n",
      "(Iteration 2347 / 4215) loss: 2.297380\n",
      "(Iteration 2349 / 4215) loss: 2.298374\n",
      "(Iteration 2351 / 4215) loss: 2.290606\n",
      "(Iteration 2353 / 4215) loss: 2.297552\n",
      "(Iteration 2355 / 4215) loss: 2.303662\n",
      "(Iteration 2357 / 4215) loss: 2.302123\n",
      "(Iteration 2359 / 4215) loss: 2.299378\n",
      "(Iteration 2361 / 4215) loss: 2.299117\n",
      "(Iteration 2363 / 4215) loss: 2.300929\n",
      "(Iteration 2365 / 4215) loss: 2.292055\n",
      "(Iteration 2367 / 4215) loss: 2.289437\n",
      "(Iteration 2369 / 4215) loss: 2.288761\n",
      "(Iteration 2371 / 4215) loss: 2.292275\n",
      "(Iteration 2373 / 4215) loss: 2.304247\n",
      "(Iteration 2375 / 4215) loss: 2.294253\n",
      "(Iteration 2377 / 4215) loss: 2.301687\n",
      "(Iteration 2379 / 4215) loss: 2.296600\n",
      "(Iteration 2381 / 4215) loss: 2.299735\n",
      "(Iteration 2383 / 4215) loss: 2.301117\n",
      "(Iteration 2385 / 4215) loss: 2.295022\n",
      "(Iteration 2387 / 4215) loss: 2.305845\n",
      "(Iteration 2389 / 4215) loss: 2.302325\n",
      "(Iteration 2391 / 4215) loss: 2.294425\n",
      "(Iteration 2393 / 4215) loss: 2.294357\n",
      "(Iteration 2395 / 4215) loss: 2.300699\n",
      "(Iteration 2397 / 4215) loss: 2.300759\n",
      "(Iteration 2399 / 4215) loss: 2.304378\n",
      "(Iteration 2401 / 4215) loss: 2.304400\n",
      "(Iteration 2403 / 4215) loss: 2.311721\n",
      "(Iteration 2405 / 4215) loss: 2.303387\n",
      "(Iteration 2407 / 4215) loss: 2.298652\n",
      "(Iteration 2409 / 4215) loss: 2.306713\n",
      "(Iteration 2411 / 4215) loss: 2.299939\n",
      "(Iteration 2413 / 4215) loss: 2.304891\n",
      "(Iteration 2415 / 4215) loss: 2.309298\n",
      "(Iteration 2417 / 4215) loss: 2.303165\n",
      "(Iteration 2419 / 4215) loss: 2.299532\n",
      "(Iteration 2421 / 4215) loss: 2.300751\n",
      "(Iteration 2423 / 4215) loss: 2.302947\n",
      "(Iteration 2425 / 4215) loss: 2.301405\n",
      "(Iteration 2427 / 4215) loss: 2.299536\n",
      "(Iteration 2429 / 4215) loss: 2.303278\n",
      "(Iteration 2431 / 4215) loss: 2.286094\n",
      "(Iteration 2433 / 4215) loss: 2.304048\n",
      "(Iteration 2435 / 4215) loss: 2.303256\n",
      "(Iteration 2437 / 4215) loss: 2.301799\n",
      "(Iteration 2439 / 4215) loss: 2.298167\n",
      "(Iteration 2441 / 4215) loss: 2.293611\n",
      "(Iteration 2443 / 4215) loss: 2.298692\n",
      "(Iteration 2445 / 4215) loss: 2.304658\n",
      "(Iteration 2447 / 4215) loss: 2.302078\n",
      "(Iteration 2449 / 4215) loss: 2.292381\n",
      "(Iteration 2451 / 4215) loss: 2.301776\n",
      "(Iteration 2453 / 4215) loss: 2.300613\n",
      "(Iteration 2455 / 4215) loss: 2.295336\n",
      "(Iteration 2457 / 4215) loss: 2.301733\n",
      "(Iteration 2459 / 4215) loss: 2.302749\n",
      "(Iteration 2461 / 4215) loss: 2.305307\n",
      "(Iteration 2463 / 4215) loss: 2.311765\n",
      "(Iteration 2465 / 4215) loss: 2.302249\n",
      "(Iteration 2467 / 4215) loss: 2.296144\n",
      "(Iteration 2469 / 4215) loss: 2.304083\n",
      "(Iteration 2471 / 4215) loss: 2.301511\n",
      "(Iteration 2473 / 4215) loss: 2.308945\n",
      "(Iteration 2475 / 4215) loss: 2.297754\n",
      "(Iteration 2477 / 4215) loss: 2.303448\n",
      "(Iteration 2479 / 4215) loss: 2.299086\n",
      "(Iteration 2481 / 4215) loss: 2.302731\n",
      "(Iteration 2483 / 4215) loss: 2.303319\n",
      "(Iteration 2485 / 4215) loss: 2.297077\n",
      "(Iteration 2487 / 4215) loss: 2.302583\n",
      "(Iteration 2489 / 4215) loss: 2.305170\n",
      "(Iteration 2491 / 4215) loss: 2.299678\n",
      "(Iteration 2493 / 4215) loss: 2.309656\n",
      "(Iteration 2495 / 4215) loss: 2.305036\n",
      "(Iteration 2497 / 4215) loss: 2.300763\n",
      "(Iteration 2499 / 4215) loss: 2.299855\n",
      "(Iteration 2501 / 4215) loss: 2.290696\n",
      "(Iteration 2503 / 4215) loss: 2.302993\n",
      "(Iteration 2505 / 4215) loss: 2.298837\n",
      "(Iteration 2507 / 4215) loss: 2.300853\n",
      "(Iteration 2509 / 4215) loss: 2.296367\n",
      "(Iteration 2511 / 4215) loss: 2.309181\n",
      "(Iteration 2513 / 4215) loss: 2.300973\n",
      "(Iteration 2515 / 4215) loss: 2.301856\n",
      "(Iteration 2517 / 4215) loss: 2.289427\n",
      "(Iteration 2519 / 4215) loss: 2.305932\n",
      "(Iteration 2521 / 4215) loss: 2.295131\n",
      "(Iteration 2523 / 4215) loss: 2.294739\n",
      "(Iteration 2525 / 4215) loss: 2.312628\n",
      "(Iteration 2527 / 4215) loss: 2.308841\n",
      "(Iteration 2529 / 4215) loss: 2.303570\n",
      "(Epoch 3 / 5) train acc: 0.108000; val_acc: 0.115000\n",
      "(Iteration 2531 / 4215) loss: 2.301825\n",
      "(Iteration 2533 / 4215) loss: 2.313366\n",
      "(Iteration 2535 / 4215) loss: 2.295463\n",
      "(Iteration 2537 / 4215) loss: 2.301144\n",
      "(Iteration 2539 / 4215) loss: 2.294774\n",
      "(Iteration 2541 / 4215) loss: 2.292820\n",
      "(Iteration 2543 / 4215) loss: 2.296848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 2545 / 4215) loss: 2.309316\n",
      "(Iteration 2547 / 4215) loss: 2.300835\n",
      "(Iteration 2549 / 4215) loss: 2.299839\n",
      "(Iteration 2551 / 4215) loss: 2.303345\n",
      "(Iteration 2553 / 4215) loss: 2.307612\n",
      "(Iteration 2555 / 4215) loss: 2.301802\n",
      "(Iteration 2557 / 4215) loss: 2.300417\n",
      "(Iteration 2559 / 4215) loss: 2.300812\n",
      "(Iteration 2561 / 4215) loss: 2.301822\n",
      "(Iteration 2563 / 4215) loss: 2.303908\n",
      "(Iteration 2565 / 4215) loss: 2.294326\n",
      "(Iteration 2567 / 4215) loss: 2.301925\n",
      "(Iteration 2569 / 4215) loss: 2.303468\n",
      "(Iteration 2571 / 4215) loss: 2.297286\n",
      "(Iteration 2573 / 4215) loss: 2.294814\n",
      "(Iteration 2575 / 4215) loss: 2.302633\n",
      "(Iteration 2577 / 4215) loss: 2.301561\n",
      "(Iteration 2579 / 4215) loss: 2.304898\n",
      "(Iteration 2581 / 4215) loss: 2.302024\n",
      "(Iteration 2583 / 4215) loss: 2.306399\n",
      "(Iteration 2585 / 4215) loss: 2.305033\n",
      "(Iteration 2587 / 4215) loss: 2.306367\n",
      "(Iteration 2589 / 4215) loss: 2.299455\n",
      "(Iteration 2591 / 4215) loss: 2.309684\n",
      "(Iteration 2593 / 4215) loss: 2.297804\n",
      "(Iteration 2595 / 4215) loss: 2.297458\n",
      "(Iteration 2597 / 4215) loss: 2.290058\n",
      "(Iteration 2599 / 4215) loss: 2.310106\n",
      "(Iteration 2601 / 4215) loss: 2.306756\n",
      "(Iteration 2603 / 4215) loss: 2.296291\n",
      "(Iteration 2605 / 4215) loss: 2.307120\n",
      "(Iteration 2607 / 4215) loss: 2.302596\n",
      "(Iteration 2609 / 4215) loss: 2.302862\n",
      "(Iteration 2611 / 4215) loss: 2.298699\n",
      "(Iteration 2613 / 4215) loss: 2.304196\n",
      "(Iteration 2615 / 4215) loss: 2.297819\n",
      "(Iteration 2617 / 4215) loss: 2.297243\n",
      "(Iteration 2619 / 4215) loss: 2.297009\n",
      "(Iteration 2621 / 4215) loss: 2.312055\n",
      "(Iteration 2623 / 4215) loss: 2.309589\n",
      "(Iteration 2625 / 4215) loss: 2.300113\n",
      "(Iteration 2627 / 4215) loss: 2.301690\n",
      "(Iteration 2629 / 4215) loss: 2.302301\n",
      "(Iteration 2631 / 4215) loss: 2.294370\n",
      "(Iteration 2633 / 4215) loss: 2.286198\n",
      "(Iteration 2635 / 4215) loss: 2.298201\n",
      "(Iteration 2637 / 4215) loss: 2.293970\n",
      "(Iteration 2639 / 4215) loss: 2.300775\n",
      "(Iteration 2641 / 4215) loss: 2.302538\n",
      "(Iteration 2643 / 4215) loss: 2.298804\n",
      "(Iteration 2645 / 4215) loss: 2.297381\n",
      "(Iteration 2647 / 4215) loss: 2.301842\n",
      "(Iteration 2649 / 4215) loss: 2.306266\n",
      "(Iteration 2651 / 4215) loss: 2.297281\n",
      "(Iteration 2653 / 4215) loss: 2.294175\n",
      "(Iteration 2655 / 4215) loss: 2.286761\n",
      "(Iteration 2657 / 4215) loss: 2.305172\n",
      "(Iteration 2659 / 4215) loss: 2.302030\n",
      "(Iteration 2661 / 4215) loss: 2.306077\n",
      "(Iteration 2663 / 4215) loss: 2.306101\n",
      "(Iteration 2665 / 4215) loss: 2.302173\n",
      "(Iteration 2667 / 4215) loss: 2.297010\n",
      "(Iteration 2669 / 4215) loss: 2.306341\n",
      "(Iteration 2671 / 4215) loss: 2.298310\n",
      "(Iteration 2673 / 4215) loss: 2.298254\n",
      "(Iteration 2675 / 4215) loss: 2.300288\n",
      "(Iteration 2677 / 4215) loss: 2.303283\n",
      "(Iteration 2679 / 4215) loss: 2.296659\n",
      "(Iteration 2681 / 4215) loss: 2.303757\n",
      "(Iteration 2683 / 4215) loss: 2.315786\n",
      "(Iteration 2685 / 4215) loss: 2.299583\n",
      "(Iteration 2687 / 4215) loss: 2.315797\n",
      "(Iteration 2689 / 4215) loss: 2.302245\n",
      "(Iteration 2691 / 4215) loss: 2.302642\n",
      "(Iteration 2693 / 4215) loss: 2.297852\n",
      "(Iteration 2695 / 4215) loss: 2.306137\n",
      "(Iteration 2697 / 4215) loss: 2.297885\n",
      "(Iteration 2699 / 4215) loss: 2.314226\n",
      "(Iteration 2701 / 4215) loss: 2.294813\n",
      "(Iteration 2703 / 4215) loss: 2.292284\n",
      "(Iteration 2705 / 4215) loss: 2.309369\n",
      "(Iteration 2707 / 4215) loss: 2.303240\n",
      "(Iteration 2709 / 4215) loss: 2.299354\n",
      "(Iteration 2711 / 4215) loss: 2.304934\n",
      "(Iteration 2713 / 4215) loss: 2.302316\n",
      "(Iteration 2715 / 4215) loss: 2.297333\n",
      "(Iteration 2717 / 4215) loss: 2.300287\n",
      "(Iteration 2719 / 4215) loss: 2.298580\n",
      "(Iteration 2721 / 4215) loss: 2.301445\n",
      "(Iteration 2723 / 4215) loss: 2.294790\n",
      "(Iteration 2725 / 4215) loss: 2.306370\n",
      "(Iteration 2727 / 4215) loss: 2.308732\n",
      "(Iteration 2729 / 4215) loss: 2.291084\n",
      "(Iteration 2731 / 4215) loss: 2.298815\n",
      "(Iteration 2733 / 4215) loss: 2.310072\n",
      "(Iteration 2735 / 4215) loss: 2.299319\n",
      "(Iteration 2737 / 4215) loss: 2.295330\n",
      "(Iteration 2739 / 4215) loss: 2.302038\n",
      "(Iteration 2741 / 4215) loss: 2.299238\n",
      "(Iteration 2743 / 4215) loss: 2.310500\n",
      "(Iteration 2745 / 4215) loss: 2.299544\n",
      "(Iteration 2747 / 4215) loss: 2.299169\n",
      "(Iteration 2749 / 4215) loss: 2.304652\n",
      "(Iteration 2751 / 4215) loss: 2.310119\n",
      "(Iteration 2753 / 4215) loss: 2.306524\n",
      "(Iteration 2755 / 4215) loss: 2.298358\n",
      "(Iteration 2757 / 4215) loss: 2.296753\n",
      "(Iteration 2759 / 4215) loss: 2.298036\n",
      "(Iteration 2761 / 4215) loss: 2.297410\n",
      "(Iteration 2763 / 4215) loss: 2.296421\n",
      "(Iteration 2765 / 4215) loss: 2.319579\n",
      "(Iteration 2767 / 4215) loss: 2.319347\n",
      "(Iteration 2769 / 4215) loss: 2.297651\n",
      "(Iteration 2771 / 4215) loss: 2.303216\n",
      "(Iteration 2773 / 4215) loss: 2.294701\n",
      "(Iteration 2775 / 4215) loss: 2.311971\n",
      "(Iteration 2777 / 4215) loss: 2.304504\n",
      "(Iteration 2779 / 4215) loss: 2.305396\n",
      "(Iteration 2781 / 4215) loss: 2.310491\n",
      "(Iteration 2783 / 4215) loss: 2.297767\n",
      "(Iteration 2785 / 4215) loss: 2.305580\n",
      "(Iteration 2787 / 4215) loss: 2.290056\n",
      "(Iteration 2789 / 4215) loss: 2.311565\n",
      "(Iteration 2791 / 4215) loss: 2.306592\n",
      "(Iteration 2793 / 4215) loss: 2.304199\n"
     ]
    }
   ],
   "source": [
    "# CNN - no batch norm or dropout\n",
    "import cnn\n",
    "d = X_train_full[1].shape\n",
    "cnn_model = cnn.ConvNet(input_dim=d, num_filters=8, filter_size=3, \\\n",
    "            hidden_dim=100, num_classes=10, weight_scale=1e-3, reg=0.0, dtype=np.float32)\n",
    "config = {'learning_rate':1e-2}\n",
    "cnn_solv = solver.Solver(cnn_model, data_mnist, optim_config = config,\n",
    "                  num_epochs=5, batch_size=64, print_every=2)\n",
    "cnn_solv.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
