{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load MNIST Data\n",
    "import numpy as np\n",
    "from layers import *\n",
    "import solver\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                   (0.5, 0.5), (0.5, 0.5))\n",
    "                             ])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=60000, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(trainset, batch_size=10000, shuffle=True)\n",
    "\n",
    "classes = ('0', '1', '2', '3',\n",
    "           '4', '5', '6', '7', '8', '9')\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "X_train_full, y_train_full = dataiter.next()\n",
    "X_train_full = X_train_full.numpy()\n",
    "y_train_full = y_train_full.numpy()\n",
    "X_train_full = X_train_full.reshape(60000, 28*28)\n",
    "\n",
    "n_train = int(np.round(0.9*X_train_full.shape[0]))\n",
    "d = X_train_full.shape[1]\n",
    "\n",
    "X_train = X_train_full[:n_train]\n",
    "X_val = X_train_full[n_train:]\n",
    "y_train = y_train_full[:n_train]\n",
    "y_val = y_train_full[n_train:]\n",
    "data_mnist = {'X_train':X_train, 'X_val':X_val, 'y_train':y_train, 'y_val':y_val}                              \n",
    "                              \n",
    "dataiter = iter(testloader)\n",
    "X_test, y_test = dataiter.next()\n",
    "X_test = X_test.numpy()\n",
    "y_test = y_test.numpy()\n",
    "X_test = X_test.reshape(10000, 28*28)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sketch the training data\n",
    "A = np.random.randn(2000, X_train_full.shape[0])\n",
    "X_train_full_s = A @ X_train_full\n",
    "X_train_s = X_train_full_s[:-2]\n",
    "X_val_s = X_train_full_s[-1]\n",
    "y_train_full_s = A @ y_train_full\n",
    "y_train_s = y_train_full_s[:-2]\n",
    "y_val_s = y_train_full_s[-1]\n",
    "data_mnist_s = {'X_train':X_train_s, 'X_val':X_val_s, 'y_train':y_train_s, 'y_val':y_val_s}\n",
    "d = X_train_full_s.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 84300) loss: 34.562966\n",
      "(Epoch 0 / 100) train acc: 0.105000; val_acc: 0.117333\n",
      "(Iteration 101 / 84300) loss: 259881.220379\n",
      "(Iteration 201 / 84300) loss: 1055579.485308\n",
      "(Iteration 301 / 84300) loss: 2339215.731934\n",
      "(Iteration 401 / 84300) loss: 4249701.282463\n",
      "(Iteration 501 / 84300) loss: 6717560.132882\n",
      "(Iteration 601 / 84300) loss: 9656308.503826\n",
      "(Iteration 701 / 84300) loss: 13342785.214187\n",
      "(Iteration 801 / 84300) loss: 16776874.567699\n",
      "(Epoch 1 / 100) train acc: 0.112000; val_acc: 0.117333\n",
      "(Iteration 901 / 84300) loss: 21751634.489095\n",
      "(Iteration 1001 / 84300) loss: 26802301.128454\n",
      "(Iteration 1101 / 84300) loss: 32374332.571673\n",
      "(Iteration 1201 / 84300) loss: 37780063.041291\n",
      "(Iteration 1301 / 84300) loss: 45047550.491834\n",
      "(Iteration 1401 / 84300) loss: 51452017.970362\n",
      "(Iteration 1501 / 84300) loss: 61412138.794138\n",
      "(Iteration 1601 / 84300) loss: 68039058.021851\n",
      "(Epoch 2 / 100) train acc: 0.099000; val_acc: 0.117333\n",
      "(Iteration 1701 / 84300) loss: 77745260.623011\n",
      "(Iteration 1801 / 84300) loss: 87486632.245106\n",
      "(Iteration 1901 / 84300) loss: 94773210.345866\n",
      "(Iteration 2001 / 84300) loss: 107090867.184067\n",
      "(Iteration 2101 / 84300) loss: 116853813.798787\n",
      "(Iteration 2201 / 84300) loss: 130548468.660737\n",
      "(Iteration 2301 / 84300) loss: 140518470.941403\n",
      "(Iteration 2401 / 84300) loss: 154317926.155141\n",
      "(Iteration 2501 / 84300) loss: 168771488.314699\n",
      "(Epoch 3 / 100) train acc: 0.120000; val_acc: 0.117333\n",
      "(Iteration 2601 / 84300) loss: 178617930.208663\n",
      "(Iteration 2701 / 84300) loss: 193147805.996215\n",
      "(Iteration 2801 / 84300) loss: 208880341.644799\n",
      "(Iteration 2901 / 84300) loss: 228101191.112595\n",
      "(Iteration 3001 / 84300) loss: 238285881.066507\n",
      "(Iteration 3101 / 84300) loss: 259855675.598626\n",
      "(Iteration 3201 / 84300) loss: 278416266.093789\n",
      "(Iteration 3301 / 84300) loss: 295612163.716411\n",
      "(Epoch 4 / 100) train acc: 0.120000; val_acc: 0.117333\n",
      "(Iteration 3401 / 84300) loss: 304669446.060957\n",
      "(Iteration 3501 / 84300) loss: 328541718.052136\n",
      "(Iteration 3601 / 84300) loss: 349604633.313067\n",
      "(Iteration 3701 / 84300) loss: 366734993.232545\n",
      "(Iteration 3801 / 84300) loss: 381561014.793132\n",
      "(Iteration 3901 / 84300) loss: 408898298.590405\n",
      "(Iteration 4001 / 84300) loss: 421455031.866876\n",
      "(Iteration 4101 / 84300) loss: 442616888.061530\n",
      "(Iteration 4201 / 84300) loss: 467986543.378931\n",
      "(Epoch 5 / 100) train acc: 0.108000; val_acc: 0.117333\n",
      "(Iteration 4301 / 84300) loss: 502742795.384887\n",
      "(Iteration 4401 / 84300) loss: 515122133.143676\n",
      "(Iteration 4501 / 84300) loss: 530658065.755740\n",
      "(Iteration 4601 / 84300) loss: 549335027.647715\n",
      "(Iteration 4701 / 84300) loss: 580524201.541936\n",
      "(Iteration 4801 / 84300) loss: 626325487.084818\n",
      "(Iteration 4901 / 84300) loss: 644415062.828785\n",
      "(Iteration 5001 / 84300) loss: 660339962.103138\n",
      "(Epoch 6 / 100) train acc: 0.109000; val_acc: 0.117333\n",
      "(Iteration 5101 / 84300) loss: 692402350.271444\n",
      "(Iteration 5201 / 84300) loss: 740672889.373796\n",
      "(Iteration 5301 / 84300) loss: 743849025.840946\n",
      "(Iteration 5401 / 84300) loss: 765876684.231628\n",
      "(Iteration 5501 / 84300) loss: 800000947.561785\n",
      "(Iteration 5601 / 84300) loss: 802938533.724499\n",
      "(Iteration 5701 / 84300) loss: 867464978.584551\n",
      "(Iteration 5801 / 84300) loss: 911243438.977723\n",
      "(Iteration 5901 / 84300) loss: 929427629.309244\n",
      "(Epoch 7 / 100) train acc: 0.106000; val_acc: 0.117333\n",
      "(Iteration 6001 / 84300) loss: 961287636.799127\n",
      "(Iteration 6101 / 84300) loss: 1007184965.976109\n",
      "(Iteration 6201 / 84300) loss: 1029432361.611196\n",
      "(Iteration 6301 / 84300) loss: 1089618624.675525\n",
      "(Iteration 6401 / 84300) loss: 1072244621.975607\n",
      "(Iteration 6501 / 84300) loss: 1128223850.656197\n",
      "(Iteration 6601 / 84300) loss: 1167700046.402479\n",
      "(Iteration 6701 / 84300) loss: 1228821233.124252\n",
      "(Epoch 8 / 100) train acc: 0.111000; val_acc: 0.117333\n",
      "(Iteration 6801 / 84300) loss: 1254012051.256020\n",
      "(Iteration 6901 / 84300) loss: 1238065634.914785\n",
      "(Iteration 7001 / 84300) loss: 1307941721.936584\n",
      "(Iteration 7101 / 84300) loss: 1321794496.003269\n",
      "(Iteration 7201 / 84300) loss: 1408635890.557872\n",
      "(Iteration 7301 / 84300) loss: 1391882583.653585\n",
      "(Iteration 7401 / 84300) loss: 1477320041.970073\n",
      "(Iteration 7501 / 84300) loss: 1492361033.891674\n",
      "(Epoch 9 / 100) train acc: 0.111000; val_acc: 0.117333\n",
      "(Iteration 7601 / 84300) loss: 1558887293.417255\n",
      "(Iteration 7701 / 84300) loss: 1614555051.538313\n",
      "(Iteration 7801 / 84300) loss: 1603694100.417472\n",
      "(Iteration 7901 / 84300) loss: 1617037791.257383\n",
      "(Iteration 8001 / 84300) loss: 1732258936.803145\n",
      "(Iteration 8101 / 84300) loss: 1710471115.322841\n",
      "(Iteration 8201 / 84300) loss: 1840381181.102462\n",
      "(Iteration 8301 / 84300) loss: 1814036162.586538\n",
      "(Iteration 8401 / 84300) loss: 1890055208.712548\n",
      "(Epoch 10 / 100) train acc: 0.113000; val_acc: 0.117333\n",
      "(Iteration 8501 / 84300) loss: 1933396611.553882\n",
      "(Iteration 8601 / 84300) loss: 1963384496.027216\n",
      "(Iteration 8701 / 84300) loss: 2002957482.079837\n",
      "(Iteration 8801 / 84300) loss: 2065045307.723383\n",
      "(Iteration 8901 / 84300) loss: 2138532527.129019\n",
      "(Iteration 9001 / 84300) loss: 2143627429.066811\n",
      "(Iteration 9101 / 84300) loss: 2215271146.669802\n",
      "(Iteration 9201 / 84300) loss: 2244558919.863830\n",
      "(Epoch 11 / 100) train acc: 0.107000; val_acc: 0.117333\n",
      "(Iteration 9301 / 84300) loss: 2292825367.126006\n",
      "(Iteration 9401 / 84300) loss: 2366536634.340160\n",
      "(Iteration 9501 / 84300) loss: 2441735389.927651\n",
      "(Iteration 9601 / 84300) loss: 2520024663.133800\n",
      "(Iteration 9701 / 84300) loss: 2484450609.675713\n",
      "(Iteration 9801 / 84300) loss: 2576706312.868643\n",
      "(Iteration 9901 / 84300) loss: 2619383995.914709\n",
      "(Iteration 10001 / 84300) loss: 2678196612.646724\n",
      "(Iteration 10101 / 84300) loss: 2761154552.209498\n",
      "(Epoch 12 / 100) train acc: 0.115000; val_acc: 0.117333\n",
      "(Iteration 10201 / 84300) loss: 2754747566.832994\n",
      "(Iteration 10301 / 84300) loss: 2778016023.702586\n",
      "(Iteration 10401 / 84300) loss: 2916216765.630577\n",
      "(Iteration 10501 / 84300) loss: 2936467105.542645\n",
      "(Iteration 10601 / 84300) loss: 3010250414.714123\n",
      "(Iteration 10701 / 84300) loss: 3105700079.934270\n",
      "(Iteration 10801 / 84300) loss: 3075128753.780504\n",
      "(Iteration 10901 / 84300) loss: 3223052912.298491\n",
      "(Epoch 13 / 100) train acc: 0.111000; val_acc: 0.117333\n",
      "(Iteration 11001 / 84300) loss: 3103480771.606906\n",
      "(Iteration 11101 / 84300) loss: 3258719729.805257\n",
      "(Iteration 11201 / 84300) loss: 3286677480.442885\n",
      "(Iteration 11301 / 84300) loss: 3491814015.704598\n",
      "(Iteration 11401 / 84300) loss: 3424635892.214157\n",
      "(Iteration 11501 / 84300) loss: 3568423421.991667\n",
      "(Iteration 11601 / 84300) loss: 3609538329.309145\n",
      "(Iteration 11701 / 84300) loss: 3515260162.805751\n",
      "(Iteration 11801 / 84300) loss: 3619923555.973375\n",
      "(Epoch 14 / 100) train acc: 0.097000; val_acc: 0.117333\n",
      "(Iteration 11901 / 84300) loss: 3816689670.291351\n",
      "(Iteration 12001 / 84300) loss: 3825258586.175112\n",
      "(Iteration 12101 / 84300) loss: 4017483699.317599\n",
      "(Iteration 12201 / 84300) loss: 3978042993.709402\n",
      "(Iteration 12301 / 84300) loss: 4074493313.538507\n",
      "(Iteration 12401 / 84300) loss: 4157684153.399127\n",
      "(Iteration 12501 / 84300) loss: 4059284439.061086\n",
      "(Iteration 12601 / 84300) loss: 4190475794.172677\n",
      "(Epoch 15 / 100) train acc: 0.107000; val_acc: 0.117333\n",
      "(Iteration 12701 / 84300) loss: 4238356210.544777\n",
      "(Iteration 12801 / 84300) loss: 4469014827.282441\n",
      "(Iteration 12901 / 84300) loss: 4547706218.244152\n",
      "(Iteration 13001 / 84300) loss: 4521728169.335127\n",
      "(Iteration 13101 / 84300) loss: 4460433157.929546\n",
      "(Iteration 13201 / 84300) loss: 4646422448.908871\n",
      "(Iteration 13301 / 84300) loss: 4757750167.271358\n",
      "(Iteration 13401 / 84300) loss: 4617070485.097647\n",
      "(Epoch 16 / 100) train acc: 0.121000; val_acc: 0.117333\n",
      "(Iteration 13501 / 84300) loss: 4986697752.602702\n",
      "(Iteration 13601 / 84300) loss: 4859028388.817429\n",
      "(Iteration 13701 / 84300) loss: 5104697990.837988\n",
      "(Iteration 13801 / 84300) loss: 5100513907.806112\n",
      "(Iteration 13901 / 84300) loss: 5168645353.726564\n",
      "(Iteration 14001 / 84300) loss: 5166950067.295079\n",
      "(Iteration 14101 / 84300) loss: 5208360799.469969\n",
      "(Iteration 14201 / 84300) loss: 5234236542.090481\n",
      "(Iteration 14301 / 84300) loss: 5483379645.740513\n",
      "(Epoch 17 / 100) train acc: 0.120000; val_acc: 0.117333\n",
      "(Iteration 14401 / 84300) loss: 5450138239.408045\n",
      "(Iteration 14501 / 84300) loss: 5574560935.442945\n",
      "(Iteration 14601 / 84300) loss: 5749864724.001579\n",
      "(Iteration 14701 / 84300) loss: 5771735489.608497\n",
      "(Iteration 14801 / 84300) loss: 5914358231.112516\n",
      "(Iteration 14901 / 84300) loss: 5904590941.712971\n",
      "(Iteration 15001 / 84300) loss: 6039712511.497482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 15101 / 84300) loss: 6101433819.797058\n",
      "(Epoch 18 / 100) train acc: 0.105000; val_acc: 0.117333\n",
      "(Iteration 15201 / 84300) loss: 6126162217.614614\n",
      "(Iteration 15301 / 84300) loss: 6275892439.426982\n",
      "(Iteration 15401 / 84300) loss: 6467352691.695419\n",
      "(Iteration 15501 / 84300) loss: 6484111284.493578\n",
      "(Iteration 15601 / 84300) loss: 6408674720.342392\n",
      "(Iteration 15701 / 84300) loss: 6569520822.596554\n",
      "(Iteration 15801 / 84300) loss: 6689876309.164126\n",
      "(Iteration 15901 / 84300) loss: 6834642906.828927\n",
      "(Iteration 16001 / 84300) loss: 7109219421.220366\n",
      "(Epoch 19 / 100) train acc: 0.102000; val_acc: 0.117333\n",
      "(Iteration 16101 / 84300) loss: 6992296582.158972\n",
      "(Iteration 16201 / 84300) loss: 7229858774.966612\n",
      "(Iteration 16301 / 84300) loss: 6969196543.629671\n",
      "(Iteration 16401 / 84300) loss: 7223676926.187672\n",
      "(Iteration 16501 / 84300) loss: 7184640355.597754\n",
      "(Iteration 16601 / 84300) loss: 7451331387.319847\n",
      "(Iteration 16701 / 84300) loss: 7541614663.954403\n",
      "(Iteration 16801 / 84300) loss: 7297120706.599438\n",
      "(Epoch 20 / 100) train acc: 0.104000; val_acc: 0.117333\n",
      "(Iteration 16901 / 84300) loss: 7487868835.873404\n",
      "(Iteration 17001 / 84300) loss: 7509363665.999507\n",
      "(Iteration 17101 / 84300) loss: 7886775664.306412\n",
      "(Iteration 17201 / 84300) loss: 7737963790.616079\n",
      "(Iteration 17301 / 84300) loss: 8075423914.284994\n",
      "(Iteration 17401 / 84300) loss: 8115952084.839654\n",
      "(Iteration 17501 / 84300) loss: 8191867411.774325\n",
      "(Iteration 17601 / 84300) loss: 8342472253.874743\n",
      "(Iteration 17701 / 84300) loss: 8165036818.093679\n",
      "(Epoch 21 / 100) train acc: 0.100000; val_acc: 0.117333\n",
      "(Iteration 17801 / 84300) loss: 8407740506.401772\n",
      "(Iteration 17901 / 84300) loss: 8413958833.481735\n",
      "(Iteration 18001 / 84300) loss: 8762521464.459457\n",
      "(Iteration 18101 / 84300) loss: 8780045913.277836\n",
      "(Iteration 18201 / 84300) loss: 9018215200.104134\n",
      "(Iteration 18301 / 84300) loss: 8607964661.986572\n",
      "(Iteration 18401 / 84300) loss: 8925557284.436287\n",
      "(Iteration 18501 / 84300) loss: 8928652654.379635\n",
      "(Epoch 22 / 100) train acc: 0.126000; val_acc: 0.117333\n",
      "(Iteration 18601 / 84300) loss: 9362368255.320290\n",
      "(Iteration 18701 / 84300) loss: 9238494110.731367\n",
      "(Iteration 18801 / 84300) loss: 9308226458.012814\n",
      "(Iteration 18901 / 84300) loss: 9636082572.680607\n",
      "(Iteration 19001 / 84300) loss: 9714301852.137114\n",
      "(Iteration 19101 / 84300) loss: 9736183508.800964\n",
      "(Iteration 19201 / 84300) loss: 9503124250.543549\n",
      "(Iteration 19301 / 84300) loss: 10075690218.425522\n",
      "(Epoch 23 / 100) train acc: 0.108000; val_acc: 0.117333\n",
      "(Iteration 19401 / 84300) loss: 9725290598.879993\n",
      "(Iteration 19501 / 84300) loss: 10115933127.487667\n",
      "(Iteration 19601 / 84300) loss: 10348238144.047943\n",
      "(Iteration 19701 / 84300) loss: 10327718289.894791\n",
      "(Iteration 19801 / 84300) loss: 10650857185.880388\n",
      "(Iteration 19901 / 84300) loss: 10817050732.350782\n",
      "(Iteration 20001 / 84300) loss: 10795795661.092512\n",
      "(Iteration 20101 / 84300) loss: 10868030967.483932\n",
      "(Iteration 20201 / 84300) loss: 10664781359.720127\n",
      "(Epoch 24 / 100) train acc: 0.098000; val_acc: 0.117333\n",
      "(Iteration 20301 / 84300) loss: 10796851765.790688\n",
      "(Iteration 20401 / 84300) loss: 11277673972.721920\n",
      "(Iteration 20501 / 84300) loss: 11066622888.250526\n",
      "(Iteration 20601 / 84300) loss: 11582607433.413343\n",
      "(Iteration 20701 / 84300) loss: 11248766881.648628\n",
      "(Iteration 20801 / 84300) loss: 11424021774.587177\n",
      "(Iteration 20901 / 84300) loss: 11738267812.465885\n",
      "(Iteration 21001 / 84300) loss: 11681883109.300491\n",
      "(Epoch 25 / 100) train acc: 0.128000; val_acc: 0.117333\n",
      "(Iteration 21101 / 84300) loss: 11817527048.355232\n",
      "(Iteration 21201 / 84300) loss: 11822602065.450600\n",
      "(Iteration 21301 / 84300) loss: 12310828949.679520\n",
      "(Iteration 21401 / 84300) loss: 12247224277.721233\n",
      "(Iteration 21501 / 84300) loss: 12236147717.505890\n",
      "(Iteration 21601 / 84300) loss: 12427052967.535069\n",
      "(Iteration 21701 / 84300) loss: 12627283880.771870\n",
      "(Iteration 21801 / 84300) loss: 12652796720.099266\n",
      "(Iteration 21901 / 84300) loss: 12694578315.095161\n",
      "(Epoch 26 / 100) train acc: 0.109000; val_acc: 0.117333\n",
      "(Iteration 22001 / 84300) loss: 12815886937.168037\n",
      "(Iteration 22101 / 84300) loss: 12954629083.890591\n",
      "(Iteration 22201 / 84300) loss: 13122432200.804817\n",
      "(Iteration 22301 / 84300) loss: 12987887597.602892\n",
      "(Iteration 22401 / 84300) loss: 13129156680.490231\n",
      "(Iteration 22501 / 84300) loss: 13657715126.435734\n",
      "(Iteration 22601 / 84300) loss: 13473649095.273975\n",
      "(Iteration 22701 / 84300) loss: 13792443981.578413\n",
      "(Epoch 27 / 100) train acc: 0.133000; val_acc: 0.117333\n",
      "(Iteration 22801 / 84300) loss: 13799498947.004333\n",
      "(Iteration 22901 / 84300) loss: 14123889721.787064\n",
      "(Iteration 23001 / 84300) loss: 14179810345.505247\n",
      "(Iteration 23101 / 84300) loss: 14550983052.926687\n",
      "(Iteration 23201 / 84300) loss: 14373150763.614426\n",
      "(Iteration 23301 / 84300) loss: 14113342510.900772\n",
      "(Iteration 23401 / 84300) loss: 14280576234.457056\n",
      "(Iteration 23501 / 84300) loss: 14453465298.977396\n",
      "(Iteration 23601 / 84300) loss: 14693595875.746666\n",
      "(Epoch 28 / 100) train acc: 0.105000; val_acc: 0.117333\n",
      "(Iteration 23701 / 84300) loss: 14901840029.996666\n",
      "(Iteration 23801 / 84300) loss: 15198089732.250191\n",
      "(Iteration 23901 / 84300) loss: 15272252785.901684\n",
      "(Iteration 24001 / 84300) loss: 14971617609.921213\n",
      "(Iteration 24101 / 84300) loss: 15183473914.988972\n",
      "(Iteration 24201 / 84300) loss: 16017148105.318054\n",
      "(Iteration 24301 / 84300) loss: 15420035462.902557\n",
      "(Iteration 24401 / 84300) loss: 15837480643.538784\n",
      "(Epoch 29 / 100) train acc: 0.121000; val_acc: 0.117333\n",
      "(Iteration 24501 / 84300) loss: 16064910689.689365\n",
      "(Iteration 24601 / 84300) loss: 15960654065.338577\n",
      "(Iteration 24701 / 84300) loss: 15974868307.894108\n",
      "(Iteration 24801 / 84300) loss: 16751703320.339281\n",
      "(Iteration 24901 / 84300) loss: 16321408461.670233\n",
      "(Iteration 25001 / 84300) loss: 16645083301.556767\n",
      "(Iteration 25101 / 84300) loss: 16540949505.931313\n",
      "(Iteration 25201 / 84300) loss: 17137798399.987110\n",
      "(Epoch 30 / 100) train acc: 0.120000; val_acc: 0.117333\n",
      "(Iteration 25301 / 84300) loss: 17249871112.283207\n",
      "(Iteration 25401 / 84300) loss: 17340377036.462685\n",
      "(Iteration 25501 / 84300) loss: 17597899559.911346\n",
      "(Iteration 25601 / 84300) loss: 17680182045.635738\n",
      "(Iteration 25701 / 84300) loss: 17773954620.485527\n",
      "(Iteration 25801 / 84300) loss: 17463604613.519569\n",
      "(Iteration 25901 / 84300) loss: 18328633243.527451\n",
      "(Iteration 26001 / 84300) loss: 17729417455.818291\n",
      "(Iteration 26101 / 84300) loss: 17989546917.924423\n",
      "(Epoch 31 / 100) train acc: 0.100000; val_acc: 0.117333\n",
      "(Iteration 26201 / 84300) loss: 18414833618.852867\n",
      "(Iteration 26301 / 84300) loss: 18438677321.869110\n",
      "(Iteration 26401 / 84300) loss: 18541543365.417770\n",
      "(Iteration 26501 / 84300) loss: 18923068003.217949\n",
      "(Iteration 26601 / 84300) loss: 18761379541.494045\n",
      "(Iteration 26701 / 84300) loss: 19355568055.017487\n",
      "(Iteration 26801 / 84300) loss: 18978219291.992416\n",
      "(Iteration 26901 / 84300) loss: 19558757418.559769\n",
      "(Epoch 32 / 100) train acc: 0.126000; val_acc: 0.117333\n",
      "(Iteration 27001 / 84300) loss: 19230310875.999092\n",
      "(Iteration 27101 / 84300) loss: 19448932017.123215\n",
      "(Iteration 27201 / 84300) loss: 19245251829.744347\n",
      "(Iteration 27301 / 84300) loss: 19818239538.094517\n",
      "(Iteration 27401 / 84300) loss: 19736521241.974983\n",
      "(Iteration 27501 / 84300) loss: 19646851000.490204\n",
      "(Iteration 27601 / 84300) loss: 20644373177.650467\n",
      "(Iteration 27701 / 84300) loss: 20879624194.265762\n",
      "(Iteration 27801 / 84300) loss: 20268175790.300125\n",
      "(Epoch 33 / 100) train acc: 0.101000; val_acc: 0.117333\n",
      "(Iteration 27901 / 84300) loss: 21355915204.134052\n",
      "(Iteration 28001 / 84300) loss: 20944625694.922371\n",
      "(Iteration 28101 / 84300) loss: 21224627758.233971\n",
      "(Iteration 28201 / 84300) loss: 21057978412.357494\n",
      "(Iteration 28301 / 84300) loss: 20576610511.936638\n",
      "(Iteration 28401 / 84300) loss: 20895815927.423038\n",
      "(Iteration 28501 / 84300) loss: 22053243978.069260\n",
      "(Iteration 28601 / 84300) loss: 21390301985.428780\n",
      "(Epoch 34 / 100) train acc: 0.097000; val_acc: 0.117333\n",
      "(Iteration 28701 / 84300) loss: 21784774234.345757\n",
      "(Iteration 28801 / 84300) loss: 22142969870.500797\n",
      "(Iteration 28901 / 84300) loss: 22482966447.481129\n",
      "(Iteration 29001 / 84300) loss: 22726381401.717392\n",
      "(Iteration 29101 / 84300) loss: 22351472052.442986\n",
      "(Iteration 29201 / 84300) loss: 22171475596.090199\n",
      "(Iteration 29301 / 84300) loss: 22717835127.723846\n",
      "(Iteration 29401 / 84300) loss: 23996266106.738155\n",
      "(Iteration 29501 / 84300) loss: 22784784262.981258\n",
      "(Epoch 35 / 100) train acc: 0.108000; val_acc: 0.117333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 29601 / 84300) loss: 23412671228.851814\n",
      "(Iteration 29701 / 84300) loss: 23304692106.623596\n",
      "(Iteration 29801 / 84300) loss: 24567555429.480156\n",
      "(Iteration 29901 / 84300) loss: 23607823879.754074\n",
      "(Iteration 30001 / 84300) loss: 24675536565.005524\n",
      "(Iteration 30101 / 84300) loss: 23883787362.904060\n",
      "(Iteration 30201 / 84300) loss: 24822694830.336548\n",
      "(Iteration 30301 / 84300) loss: 24989815073.638714\n",
      "(Epoch 36 / 100) train acc: 0.112000; val_acc: 0.117333\n",
      "(Iteration 30401 / 84300) loss: 24841678639.822601\n",
      "(Iteration 30501 / 84300) loss: 24892378929.280914\n",
      "(Iteration 30601 / 84300) loss: 25126681158.050041\n",
      "(Iteration 30701 / 84300) loss: 25988939061.125431\n",
      "(Iteration 30801 / 84300) loss: 24671046574.262360\n",
      "(Iteration 30901 / 84300) loss: 25366561645.165337\n",
      "(Iteration 31001 / 84300) loss: 25674818091.841396\n",
      "(Iteration 31101 / 84300) loss: 25474288045.430565\n",
      "(Epoch 37 / 100) train acc: 0.121000; val_acc: 0.117333\n",
      "(Iteration 31201 / 84300) loss: 26122674596.984444\n",
      "(Iteration 31301 / 84300) loss: 26418964716.808846\n",
      "(Iteration 31401 / 84300) loss: 25539213199.623322\n",
      "(Iteration 31501 / 84300) loss: 26829626765.705524\n",
      "(Iteration 31601 / 84300) loss: 26618283169.080124\n",
      "(Iteration 31701 / 84300) loss: 27097686065.800842\n",
      "(Iteration 31801 / 84300) loss: 26490179912.470619\n",
      "(Iteration 31901 / 84300) loss: 26547935861.744236\n",
      "(Iteration 32001 / 84300) loss: 27416462260.090332\n",
      "(Epoch 38 / 100) train acc: 0.109000; val_acc: 0.117333\n",
      "(Iteration 32101 / 84300) loss: 27777217580.302441\n",
      "(Iteration 32201 / 84300) loss: 26945967220.809536\n",
      "(Iteration 32301 / 84300) loss: 27722560925.749306\n",
      "(Iteration 32401 / 84300) loss: 27496963660.475769\n",
      "(Iteration 32501 / 84300) loss: 27322795916.905685\n",
      "(Iteration 32601 / 84300) loss: 27928236881.812294\n",
      "(Iteration 32701 / 84300) loss: 28476782108.872066\n",
      "(Iteration 32801 / 84300) loss: 29050940388.080803\n",
      "(Epoch 39 / 100) train acc: 0.123000; val_acc: 0.117333\n",
      "(Iteration 32901 / 84300) loss: 29386970806.433994\n",
      "(Iteration 33001 / 84300) loss: 29073003507.795120\n",
      "(Iteration 33101 / 84300) loss: 28883975104.888927\n",
      "(Iteration 33201 / 84300) loss: 29249466075.212151\n",
      "(Iteration 33301 / 84300) loss: 29339258576.577705\n",
      "(Iteration 33401 / 84300) loss: 30071171697.697319\n",
      "(Iteration 33501 / 84300) loss: 29339695607.742065\n",
      "(Iteration 33601 / 84300) loss: 30647434927.331169\n",
      "(Iteration 33701 / 84300) loss: 30390995649.857346\n",
      "(Epoch 40 / 100) train acc: 0.098000; val_acc: 0.117333\n",
      "(Iteration 33801 / 84300) loss: 31152829844.679169\n",
      "(Iteration 33901 / 84300) loss: 29836012854.187611\n",
      "(Iteration 34001 / 84300) loss: 30783435861.752159\n",
      "(Iteration 34101 / 84300) loss: 30038183042.147598\n",
      "(Iteration 34201 / 84300) loss: 30879625674.364395\n",
      "(Iteration 34301 / 84300) loss: 31957546969.529488\n",
      "(Iteration 34401 / 84300) loss: 31574404309.571877\n",
      "(Iteration 34501 / 84300) loss: 31642403201.913620\n",
      "(Epoch 41 / 100) train acc: 0.110000; val_acc: 0.117333\n",
      "(Iteration 34601 / 84300) loss: 32081050558.030685\n",
      "(Iteration 34701 / 84300) loss: 33226025497.199692\n",
      "(Iteration 34801 / 84300) loss: 32266218118.807720\n",
      "(Iteration 34901 / 84300) loss: 32470524237.273438\n",
      "(Iteration 35001 / 84300) loss: 33191508072.742348\n",
      "(Iteration 35101 / 84300) loss: 33313732740.035469\n",
      "(Iteration 35201 / 84300) loss: 33106882587.691029\n",
      "(Iteration 35301 / 84300) loss: 32044065021.371990\n",
      "(Iteration 35401 / 84300) loss: 33294325211.064156\n",
      "(Epoch 42 / 100) train acc: 0.120000; val_acc: 0.117333\n",
      "(Iteration 35501 / 84300) loss: 33873392503.837990\n",
      "(Iteration 35601 / 84300) loss: 33318964762.610195\n",
      "(Iteration 35701 / 84300) loss: 34192665007.130219\n",
      "(Iteration 35801 / 84300) loss: 34691588430.981049\n",
      "(Iteration 35901 / 84300) loss: 33766358544.091824\n",
      "(Iteration 36001 / 84300) loss: 33495827587.897247\n",
      "(Iteration 36101 / 84300) loss: 34724204416.146019\n",
      "(Iteration 36201 / 84300) loss: 34955415557.932030\n",
      "(Epoch 43 / 100) train acc: 0.089000; val_acc: 0.117333\n",
      "(Iteration 36301 / 84300) loss: 35096748513.747154\n",
      "(Iteration 36401 / 84300) loss: 34855144590.128174\n",
      "(Iteration 36501 / 84300) loss: 35798031627.342964\n",
      "(Iteration 36601 / 84300) loss: 35442187999.997070\n",
      "(Iteration 36701 / 84300) loss: 35418187101.330032\n",
      "(Iteration 36801 / 84300) loss: 36540463151.106934\n",
      "(Iteration 36901 / 84300) loss: 36461338592.276604\n",
      "(Iteration 37001 / 84300) loss: 35855618083.954971\n",
      "(Epoch 44 / 100) train acc: 0.096000; val_acc: 0.117333\n",
      "(Iteration 37101 / 84300) loss: 36842901494.597870\n",
      "(Iteration 37201 / 84300) loss: 36056501305.398285\n",
      "(Iteration 37301 / 84300) loss: 37695629009.953697\n",
      "(Iteration 37401 / 84300) loss: 37368243158.971062\n",
      "(Iteration 37501 / 84300) loss: 38276204130.431732\n",
      "(Iteration 37601 / 84300) loss: 38060767542.410019\n",
      "(Iteration 37701 / 84300) loss: 38372211706.597939\n",
      "(Iteration 37801 / 84300) loss: 39083624678.366333\n",
      "(Iteration 37901 / 84300) loss: 38615737322.362877\n",
      "(Epoch 45 / 100) train acc: 0.106000; val_acc: 0.117333\n",
      "(Iteration 38001 / 84300) loss: 40101084518.470215\n",
      "(Iteration 38101 / 84300) loss: 38667890521.164307\n",
      "(Iteration 38201 / 84300) loss: 38751401941.566849\n",
      "(Iteration 38301 / 84300) loss: 38947402403.026001\n",
      "(Iteration 38401 / 84300) loss: 39830193215.490952\n",
      "(Iteration 38501 / 84300) loss: 39137366688.200195\n",
      "(Iteration 38601 / 84300) loss: 39930706523.363510\n",
      "(Iteration 38701 / 84300) loss: 38934647221.782501\n",
      "(Epoch 46 / 100) train acc: 0.132000; val_acc: 0.117333\n",
      "(Iteration 38801 / 84300) loss: 39394348853.601662\n",
      "(Iteration 38901 / 84300) loss: 39695263079.560226\n",
      "(Iteration 39001 / 84300) loss: 40295215118.120224\n",
      "(Iteration 39101 / 84300) loss: 40553154216.874420\n",
      "(Iteration 39201 / 84300) loss: 41544006942.610344\n",
      "(Iteration 39301 / 84300) loss: 41743615828.285561\n",
      "(Iteration 39401 / 84300) loss: 40725580677.888329\n",
      "(Iteration 39501 / 84300) loss: 41028383359.990410\n",
      "(Iteration 39601 / 84300) loss: 42004075930.458595\n",
      "(Epoch 47 / 100) train acc: 0.092000; val_acc: 0.117333\n",
      "(Iteration 39701 / 84300) loss: 42347356254.033546\n",
      "(Iteration 39801 / 84300) loss: 42245968176.317703\n",
      "(Iteration 39901 / 84300) loss: 41943480837.083794\n",
      "(Iteration 40001 / 84300) loss: 42919418309.921860\n",
      "(Iteration 40101 / 84300) loss: 43725621900.902328\n",
      "(Iteration 40201 / 84300) loss: 43289630477.891083\n",
      "(Iteration 40301 / 84300) loss: 43859149870.555374\n",
      "(Iteration 40401 / 84300) loss: 43656457079.781525\n",
      "(Epoch 48 / 100) train acc: 0.118000; val_acc: 0.117333\n",
      "(Iteration 40501 / 84300) loss: 42872568310.781212\n",
      "(Iteration 40601 / 84300) loss: 45236648763.053391\n",
      "(Iteration 40701 / 84300) loss: 44078558750.823486\n",
      "(Iteration 40801 / 84300) loss: 45307915777.467667\n",
      "(Iteration 40901 / 84300) loss: 44635656376.843155\n",
      "(Iteration 41001 / 84300) loss: 45801332042.296738\n",
      "(Iteration 41101 / 84300) loss: 44912123058.558456\n",
      "(Iteration 41201 / 84300) loss: 46258588023.576706\n",
      "(Iteration 41301 / 84300) loss: 45996071097.418991\n",
      "(Epoch 49 / 100) train acc: 0.110000; val_acc: 0.117333\n",
      "(Iteration 41401 / 84300) loss: 42998193336.508980\n",
      "(Iteration 41501 / 84300) loss: 46385558065.970291\n",
      "(Iteration 41601 / 84300) loss: 46097070289.725235\n",
      "(Iteration 41701 / 84300) loss: 45676919311.378525\n",
      "(Iteration 41801 / 84300) loss: 47764226669.051758\n",
      "(Iteration 41901 / 84300) loss: 45849126399.552406\n",
      "(Iteration 42001 / 84300) loss: 46782299357.247261\n",
      "(Iteration 42101 / 84300) loss: 48835467229.089470\n",
      "(Epoch 50 / 100) train acc: 0.099000; val_acc: 0.117333\n",
      "(Iteration 42201 / 84300) loss: 48093385083.191467\n",
      "(Iteration 42301 / 84300) loss: 46817805730.474503\n",
      "(Iteration 42401 / 84300) loss: 48599281553.407486\n",
      "(Iteration 42501 / 84300) loss: 48683563050.912964\n",
      "(Iteration 42601 / 84300) loss: 49174935351.229874\n",
      "(Iteration 42701 / 84300) loss: 48215196454.711609\n",
      "(Iteration 42801 / 84300) loss: 50088000930.568222\n",
      "(Iteration 42901 / 84300) loss: 48816449940.589462\n",
      "(Epoch 51 / 100) train acc: 0.112000; val_acc: 0.117333\n",
      "(Iteration 43001 / 84300) loss: 49733486769.669205\n",
      "(Iteration 43101 / 84300) loss: 49659854594.289490\n",
      "(Iteration 43201 / 84300) loss: 50090453493.333412\n",
      "(Iteration 43301 / 84300) loss: 48187575134.612961\n",
      "(Iteration 43401 / 84300) loss: 50095887019.153976\n",
      "(Iteration 43501 / 84300) loss: 51145402642.406761\n",
      "(Iteration 43601 / 84300) loss: 49802040918.239197\n",
      "(Iteration 43701 / 84300) loss: 50370854980.635422\n",
      "(Iteration 43801 / 84300) loss: 51668229052.134430\n",
      "(Epoch 52 / 100) train acc: 0.111000; val_acc: 0.117333\n",
      "(Iteration 43901 / 84300) loss: 50253056477.051956\n",
      "(Iteration 44001 / 84300) loss: 50670376063.518433\n",
      "(Iteration 44101 / 84300) loss: 52695425933.097801\n",
      "(Iteration 44201 / 84300) loss: 50870463075.329071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 44301 / 84300) loss: 52270834129.520752\n",
      "(Iteration 44401 / 84300) loss: 53269321883.007538\n",
      "(Iteration 44501 / 84300) loss: 52855833177.178413\n",
      "(Iteration 44601 / 84300) loss: 54261193579.001266\n",
      "(Epoch 53 / 100) train acc: 0.131000; val_acc: 0.117333\n",
      "(Iteration 44701 / 84300) loss: 52946396387.200180\n",
      "(Iteration 44801 / 84300) loss: 53844114585.429321\n",
      "(Iteration 44901 / 84300) loss: 54021688257.624199\n",
      "(Iteration 45001 / 84300) loss: 54730460766.021729\n",
      "(Iteration 45101 / 84300) loss: 55393257946.282639\n",
      "(Iteration 45201 / 84300) loss: 54867102477.820663\n",
      "(Iteration 45301 / 84300) loss: 56023571164.921982\n",
      "(Iteration 45401 / 84300) loss: 55585116278.191910\n",
      "(Iteration 45501 / 84300) loss: 54206425952.295937\n",
      "(Epoch 54 / 100) train acc: 0.096000; val_acc: 0.117333\n",
      "(Iteration 45601 / 84300) loss: 55693306286.601028\n",
      "(Iteration 45701 / 84300) loss: 54813667955.305176\n",
      "(Iteration 45801 / 84300) loss: 56991011771.969147\n",
      "(Iteration 45901 / 84300) loss: 54925160522.630081\n",
      "(Iteration 46001 / 84300) loss: 55920227068.827545\n",
      "(Iteration 46101 / 84300) loss: 57896196517.220680\n",
      "(Iteration 46201 / 84300) loss: 56466153846.863617\n",
      "(Iteration 46301 / 84300) loss: 58644208481.382416\n",
      "(Epoch 55 / 100) train acc: 0.144000; val_acc: 0.117333\n",
      "(Iteration 46401 / 84300) loss: 57950312071.970596\n",
      "(Iteration 46501 / 84300) loss: 57986763791.457291\n",
      "(Iteration 46601 / 84300) loss: 57420722184.953430\n",
      "(Iteration 46701 / 84300) loss: 56860381511.243439\n",
      "(Iteration 46801 / 84300) loss: 56970250242.420563\n",
      "(Iteration 46901 / 84300) loss: 58945253320.651482\n",
      "(Iteration 47001 / 84300) loss: 59172053622.586807\n",
      "(Iteration 47101 / 84300) loss: 59736122278.105133\n",
      "(Iteration 47201 / 84300) loss: 58051748001.656311\n",
      "(Epoch 56 / 100) train acc: 0.107000; val_acc: 0.117333\n",
      "(Iteration 47301 / 84300) loss: 59322582620.920364\n",
      "(Iteration 47401 / 84300) loss: 59572036592.128044\n",
      "(Iteration 47501 / 84300) loss: 59128180152.737129\n",
      "(Iteration 47601 / 84300) loss: 60799769421.323318\n",
      "(Iteration 47701 / 84300) loss: 60530218147.026360\n",
      "(Iteration 47801 / 84300) loss: 60995136272.022377\n",
      "(Iteration 47901 / 84300) loss: 60137728001.944458\n",
      "(Iteration 48001 / 84300) loss: 60806053520.727524\n",
      "(Epoch 57 / 100) train acc: 0.104000; val_acc: 0.117333\n",
      "(Iteration 48101 / 84300) loss: 60780356377.504318\n",
      "(Iteration 48201 / 84300) loss: 62338032382.078888\n",
      "(Iteration 48301 / 84300) loss: 60159471415.604355\n",
      "(Iteration 48401 / 84300) loss: 60189957682.649872\n",
      "(Iteration 48501 / 84300) loss: 63644854393.427223\n",
      "(Iteration 48601 / 84300) loss: 63392951861.078079\n",
      "(Iteration 48701 / 84300) loss: 62587298874.078644\n",
      "(Iteration 48801 / 84300) loss: 62610852175.231529\n",
      "(Epoch 58 / 100) train acc: 0.117000; val_acc: 0.117333\n",
      "(Iteration 48901 / 84300) loss: 63727732618.451645\n",
      "(Iteration 49001 / 84300) loss: 65054076757.851547\n",
      "(Iteration 49101 / 84300) loss: 64681519423.430923\n",
      "(Iteration 49201 / 84300) loss: 62619563386.602600\n",
      "(Iteration 49301 / 84300) loss: 66802885329.829697\n",
      "(Iteration 49401 / 84300) loss: 64512268759.370850\n",
      "(Iteration 49501 / 84300) loss: 64420808464.395493\n",
      "(Iteration 49601 / 84300) loss: 65957160992.015747\n",
      "(Iteration 49701 / 84300) loss: 66896363353.719452\n",
      "(Epoch 59 / 100) train acc: 0.114000; val_acc: 0.117333\n",
      "(Iteration 49801 / 84300) loss: 64569082809.926056\n",
      "(Iteration 49901 / 84300) loss: 66275235388.547379\n",
      "(Iteration 50001 / 84300) loss: 67918130965.750099\n",
      "(Iteration 50101 / 84300) loss: 67840662678.662323\n",
      "(Iteration 50201 / 84300) loss: 64487985289.550972\n",
      "(Iteration 50301 / 84300) loss: 69088936437.440979\n",
      "(Iteration 50401 / 84300) loss: 68249080309.770271\n",
      "(Iteration 50501 / 84300) loss: 68969215818.722000\n",
      "(Epoch 60 / 100) train acc: 0.121000; val_acc: 0.117333\n",
      "(Iteration 50601 / 84300) loss: 69591457056.806915\n",
      "(Iteration 50701 / 84300) loss: 69791985921.681717\n",
      "(Iteration 50801 / 84300) loss: 69490560136.103180\n",
      "(Iteration 50901 / 84300) loss: 68019918033.440735\n",
      "(Iteration 51001 / 84300) loss: 68756718334.202698\n",
      "(Iteration 51101 / 84300) loss: 70906248085.171326\n",
      "(Iteration 51201 / 84300) loss: 67468082565.328064\n",
      "(Iteration 51301 / 84300) loss: 69245230419.834137\n",
      "(Iteration 51401 / 84300) loss: 71297522858.376984\n",
      "(Epoch 61 / 100) train acc: 0.114000; val_acc: 0.117333\n",
      "(Iteration 51501 / 84300) loss: 72084403511.249817\n",
      "(Iteration 51601 / 84300) loss: 70597103017.375397\n",
      "(Iteration 51701 / 84300) loss: 71346554207.066177\n",
      "(Iteration 51801 / 84300) loss: 73780339717.119598\n",
      "(Iteration 51901 / 84300) loss: 72423466386.862122\n",
      "(Iteration 52001 / 84300) loss: 71089425516.352936\n",
      "(Iteration 52101 / 84300) loss: 71771463247.549408\n",
      "(Iteration 52201 / 84300) loss: 71502577638.918732\n",
      "(Epoch 62 / 100) train acc: 0.105000; val_acc: 0.117333\n",
      "(Iteration 52301 / 84300) loss: 73841857618.931061\n",
      "(Iteration 52401 / 84300) loss: 72426210981.204926\n",
      "(Iteration 52501 / 84300) loss: 73927315209.785263\n",
      "(Iteration 52601 / 84300) loss: 73131589883.370117\n",
      "(Iteration 52701 / 84300) loss: 73892344066.383392\n",
      "(Iteration 52801 / 84300) loss: 72836961955.061035\n",
      "(Iteration 52901 / 84300) loss: 77046576132.025421\n",
      "(Iteration 53001 / 84300) loss: 76034630458.330002\n",
      "(Iteration 53101 / 84300) loss: 74309195935.925720\n",
      "(Epoch 63 / 100) train acc: 0.129000; val_acc: 0.117333\n",
      "(Iteration 53201 / 84300) loss: 77447629015.394012\n",
      "(Iteration 53301 / 84300) loss: 75464663156.125854\n",
      "(Iteration 53401 / 84300) loss: 75903931446.586639\n",
      "(Iteration 53501 / 84300) loss: 75329543068.246048\n",
      "(Iteration 53601 / 84300) loss: 76945328713.856873\n",
      "(Iteration 53701 / 84300) loss: 77422496744.770111\n",
      "(Iteration 53801 / 84300) loss: 77338143508.163208\n",
      "(Iteration 53901 / 84300) loss: 76608089327.832214\n",
      "(Epoch 64 / 100) train acc: 0.126000; val_acc: 0.117333\n",
      "(Iteration 54001 / 84300) loss: 77958090373.594543\n",
      "(Iteration 54101 / 84300) loss: 77333640593.729294\n",
      "(Iteration 54201 / 84300) loss: 77649570908.994339\n",
      "(Iteration 54301 / 84300) loss: 78472927644.443878\n",
      "(Iteration 54401 / 84300) loss: 81514566870.379974\n",
      "(Iteration 54501 / 84300) loss: 80178405305.937317\n",
      "(Iteration 54601 / 84300) loss: 79844909938.225540\n",
      "(Iteration 54701 / 84300) loss: 80575020067.802933\n",
      "(Epoch 65 / 100) train acc: 0.106000; val_acc: 0.117333\n",
      "(Iteration 54801 / 84300) loss: 79044342136.981110\n",
      "(Iteration 54901 / 84300) loss: 80664560803.232941\n",
      "(Iteration 55001 / 84300) loss: 80442408211.093979\n",
      "(Iteration 55101 / 84300) loss: 81642448137.928589\n",
      "(Iteration 55201 / 84300) loss: 79740524635.745865\n",
      "(Iteration 55301 / 84300) loss: 82985417603.490112\n",
      "(Iteration 55401 / 84300) loss: 82568779768.589478\n",
      "(Iteration 55501 / 84300) loss: 81532906099.536438\n",
      "(Iteration 55601 / 84300) loss: 82141934486.800552\n",
      "(Epoch 66 / 100) train acc: 0.122000; val_acc: 0.117333\n",
      "(Iteration 55701 / 84300) loss: 83526349238.387726\n",
      "(Iteration 55801 / 84300) loss: 81809320348.151093\n",
      "(Iteration 55901 / 84300) loss: 81753807502.154785\n",
      "(Iteration 56001 / 84300) loss: 83243282864.757187\n",
      "(Iteration 56101 / 84300) loss: 86873700896.423187\n",
      "(Iteration 56201 / 84300) loss: 84589374462.096603\n",
      "(Iteration 56301 / 84300) loss: 85774860565.362381\n",
      "(Iteration 56401 / 84300) loss: 86149937922.166748\n",
      "(Epoch 67 / 100) train acc: 0.116000; val_acc: 0.117333\n",
      "(Iteration 56501 / 84300) loss: 85908101396.977661\n",
      "(Iteration 56601 / 84300) loss: 86446984765.996216\n",
      "(Iteration 56701 / 84300) loss: 87398018555.894943\n",
      "(Iteration 56801 / 84300) loss: 88760755371.255951\n",
      "(Iteration 56901 / 84300) loss: 88599411236.293732\n",
      "(Iteration 57001 / 84300) loss: 87757516538.620331\n",
      "(Iteration 57101 / 84300) loss: 88572168235.688782\n",
      "(Iteration 57201 / 84300) loss: 86035247723.372162\n",
      "(Iteration 57301 / 84300) loss: 88882189866.228363\n",
      "(Epoch 68 / 100) train acc: 0.109000; val_acc: 0.117333\n",
      "(Iteration 57401 / 84300) loss: 88843539173.230957\n",
      "(Iteration 57501 / 84300) loss: 90260687646.111664\n",
      "(Iteration 57601 / 84300) loss: 88186191524.451263\n",
      "(Iteration 57701 / 84300) loss: 90916662479.678513\n",
      "(Iteration 57801 / 84300) loss: 89336828036.993271\n",
      "(Iteration 57901 / 84300) loss: 90144143147.126007\n",
      "(Iteration 58001 / 84300) loss: 89009334777.082733\n",
      "(Iteration 58101 / 84300) loss: 90053950286.700027\n",
      "(Epoch 69 / 100) train acc: 0.112000; val_acc: 0.117333\n",
      "(Iteration 58201 / 84300) loss: 90676357097.183838\n",
      "(Iteration 58301 / 84300) loss: 88685653808.413269\n",
      "(Iteration 58401 / 84300) loss: 92655593301.524826\n",
      "(Iteration 58501 / 84300) loss: 91077325759.662811\n",
      "(Iteration 58601 / 84300) loss: 90128272183.814392\n",
      "(Iteration 58701 / 84300) loss: 92974670325.923965\n",
      "(Iteration 58801 / 84300) loss: 90408857808.045868\n",
      "(Iteration 58901 / 84300) loss: 94085370591.012039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 59001 / 84300) loss: 93824677977.589249\n",
      "(Epoch 70 / 100) train acc: 0.123000; val_acc: 0.117333\n",
      "(Iteration 59101 / 84300) loss: 94740499281.901764\n",
      "(Iteration 59201 / 84300) loss: 95285708027.851547\n",
      "(Iteration 59301 / 84300) loss: 95370370923.471680\n",
      "(Iteration 59401 / 84300) loss: 95427294977.771210\n",
      "(Iteration 59501 / 84300) loss: 95133691380.605377\n",
      "(Iteration 59601 / 84300) loss: 95790182746.431274\n",
      "(Iteration 59701 / 84300) loss: 94792287437.555603\n",
      "(Iteration 59801 / 84300) loss: 96308039454.901627\n",
      "(Epoch 71 / 100) train acc: 0.118000; val_acc: 0.117333\n",
      "(Iteration 59901 / 84300) loss: 95500988690.800476\n",
      "(Iteration 60001 / 84300) loss: 98834053435.092682\n",
      "(Iteration 60101 / 84300) loss: 95524640100.393616\n",
      "(Iteration 60201 / 84300) loss: 95426590548.117661\n",
      "(Iteration 60301 / 84300) loss: 97248863305.763336\n",
      "(Iteration 60401 / 84300) loss: 95986888261.251846\n",
      "(Iteration 60501 / 84300) loss: 95246173720.498230\n",
      "(Iteration 60601 / 84300) loss: 95650103529.536209\n",
      "(Epoch 72 / 100) train acc: 0.103000; val_acc: 0.117333\n",
      "(Iteration 60701 / 84300) loss: 97231520831.407654\n",
      "(Iteration 60801 / 84300) loss: 98701444886.232040\n",
      "(Iteration 60901 / 84300) loss: 99530799026.936432\n",
      "(Iteration 61001 / 84300) loss: 97434651008.206543\n",
      "(Iteration 61101 / 84300) loss: 102144275330.197067\n",
      "(Iteration 61201 / 84300) loss: 98693447789.508682\n",
      "(Iteration 61301 / 84300) loss: 101759131674.946472\n",
      "(Iteration 61401 / 84300) loss: 100408417211.980560\n",
      "(Iteration 61501 / 84300) loss: 103129035676.950974\n",
      "(Epoch 73 / 100) train acc: 0.107000; val_acc: 0.117333\n",
      "(Iteration 61601 / 84300) loss: 102906821188.797592\n",
      "(Iteration 61701 / 84300) loss: 101465194795.811707\n",
      "(Iteration 61801 / 84300) loss: 101334977893.627838\n",
      "(Iteration 61901 / 84300) loss: 99611066391.133209\n",
      "(Iteration 62001 / 84300) loss: 104927319843.064178\n",
      "(Iteration 62101 / 84300) loss: 102321396942.126984\n",
      "(Iteration 62201 / 84300) loss: 103756734780.215958\n",
      "(Iteration 62301 / 84300) loss: 103047678081.583984\n",
      "(Epoch 74 / 100) train acc: 0.105000; val_acc: 0.117333\n",
      "(Iteration 62401 / 84300) loss: 101745598574.669464\n",
      "(Iteration 62501 / 84300) loss: 107149854051.199631\n",
      "(Iteration 62601 / 84300) loss: 104486600841.844635\n",
      "(Iteration 62701 / 84300) loss: 103959557323.720535\n",
      "(Iteration 62801 / 84300) loss: 105011983311.306488\n",
      "(Iteration 62901 / 84300) loss: 102257185648.261902\n",
      "(Iteration 63001 / 84300) loss: 105152744505.368439\n",
      "(Iteration 63101 / 84300) loss: 106592730147.524063\n",
      "(Iteration 63201 / 84300) loss: 109279775252.050613\n",
      "(Epoch 75 / 100) train acc: 0.106000; val_acc: 0.117333\n",
      "(Iteration 63301 / 84300) loss: 104468262172.023590\n",
      "(Iteration 63401 / 84300) loss: 107459319446.059967\n",
      "(Iteration 63501 / 84300) loss: 107673433869.462341\n",
      "(Iteration 63601 / 84300) loss: 107561785615.854935\n",
      "(Iteration 63701 / 84300) loss: 108981729112.996979\n",
      "(Iteration 63801 / 84300) loss: 107959775315.197037\n",
      "(Iteration 63901 / 84300) loss: 111766035313.039505\n",
      "(Iteration 64001 / 84300) loss: 113096826628.245483\n",
      "(Epoch 76 / 100) train acc: 0.112000; val_acc: 0.117333\n",
      "(Iteration 64101 / 84300) loss: 110086935221.656128\n",
      "(Iteration 64201 / 84300) loss: 109215274425.961395\n",
      "(Iteration 64301 / 84300) loss: 107634035699.848831\n",
      "(Iteration 64401 / 84300) loss: 111626788586.404022\n",
      "(Iteration 64501 / 84300) loss: 110810307027.349213\n",
      "(Iteration 64601 / 84300) loss: 112970942687.853668\n",
      "(Iteration 64701 / 84300) loss: 113550410561.506638\n",
      "(Iteration 64801 / 84300) loss: 112582351751.677246\n",
      "(Iteration 64901 / 84300) loss: 107534833986.506683\n",
      "(Epoch 77 / 100) train acc: 0.125000; val_acc: 0.117333\n",
      "(Iteration 65001 / 84300) loss: 111754615629.930176\n",
      "(Iteration 65101 / 84300) loss: 113482994854.417633\n",
      "(Iteration 65201 / 84300) loss: 112932792148.244934\n",
      "(Iteration 65301 / 84300) loss: 113095711168.005127\n",
      "(Iteration 65401 / 84300) loss: 115152076677.661499\n",
      "(Iteration 65501 / 84300) loss: 115648568319.344696\n",
      "(Iteration 65601 / 84300) loss: 116942215395.393875\n",
      "(Iteration 65701 / 84300) loss: 114432047246.962219\n",
      "(Epoch 78 / 100) train acc: 0.119000; val_acc: 0.117333\n",
      "(Iteration 65801 / 84300) loss: 116534983008.631958\n",
      "(Iteration 65901 / 84300) loss: 116912305121.677551\n",
      "(Iteration 66001 / 84300) loss: 115174306398.326813\n",
      "(Iteration 66101 / 84300) loss: 116624934466.295853\n",
      "(Iteration 66201 / 84300) loss: 116399133422.169342\n",
      "(Iteration 66301 / 84300) loss: 118348043589.442230\n",
      "(Iteration 66401 / 84300) loss: 115116493014.045441\n",
      "(Iteration 66501 / 84300) loss: 116815096071.439148\n",
      "(Epoch 79 / 100) train acc: 0.106000; val_acc: 0.117333\n",
      "(Iteration 66601 / 84300) loss: 118791682429.396179\n",
      "(Iteration 66701 / 84300) loss: 120665024940.552704\n",
      "(Iteration 66801 / 84300) loss: 119474690072.275055\n",
      "(Iteration 66901 / 84300) loss: 121034200551.677368\n",
      "(Iteration 67001 / 84300) loss: 120302428612.319611\n",
      "(Iteration 67101 / 84300) loss: 120218499309.502670\n",
      "(Iteration 67201 / 84300) loss: 120254265166.902466\n",
      "(Iteration 67301 / 84300) loss: 122284285731.601028\n",
      "(Iteration 67401 / 84300) loss: 122717798470.199158\n",
      "(Epoch 80 / 100) train acc: 0.119000; val_acc: 0.117333\n",
      "(Iteration 67501 / 84300) loss: 116433529490.884262\n",
      "(Iteration 67601 / 84300) loss: 120798189912.596466\n",
      "(Iteration 67701 / 84300) loss: 120102050611.433960\n",
      "(Iteration 67801 / 84300) loss: 123479302181.376923\n",
      "(Iteration 67901 / 84300) loss: 123372973135.265350\n",
      "(Iteration 68001 / 84300) loss: 124907712184.501709\n",
      "(Iteration 68101 / 84300) loss: 124259599550.632446\n",
      "(Iteration 68201 / 84300) loss: 121920606559.082703\n",
      "(Epoch 81 / 100) train acc: 0.125000; val_acc: 0.117333\n",
      "(Iteration 68301 / 84300) loss: 123748780992.915619\n",
      "(Iteration 68401 / 84300) loss: 124981202295.365570\n",
      "(Iteration 68501 / 84300) loss: 125535512375.354736\n",
      "(Iteration 68601 / 84300) loss: 127057454447.170227\n",
      "(Iteration 68701 / 84300) loss: 126935253826.908844\n",
      "(Iteration 68801 / 84300) loss: 125262048159.062500\n",
      "(Iteration 68901 / 84300) loss: 123734908884.516479\n",
      "(Iteration 69001 / 84300) loss: 125335901552.367157\n",
      "(Iteration 69101 / 84300) loss: 125609760042.382736\n",
      "(Epoch 82 / 100) train acc: 0.096000; val_acc: 0.117333\n",
      "(Iteration 69201 / 84300) loss: 128988063296.942947\n",
      "(Iteration 69301 / 84300) loss: 131397674599.338745\n",
      "(Iteration 69401 / 84300) loss: 126097893532.201080\n",
      "(Iteration 69501 / 84300) loss: 131309924690.334564\n",
      "(Iteration 69601 / 84300) loss: 131691224694.059692\n",
      "(Iteration 69701 / 84300) loss: 126701704779.111130\n",
      "(Iteration 69801 / 84300) loss: 128345404773.791061\n",
      "(Iteration 69901 / 84300) loss: 131649145590.814651\n",
      "(Epoch 83 / 100) train acc: 0.114000; val_acc: 0.117333\n",
      "(Iteration 70001 / 84300) loss: 130426599351.433167\n",
      "(Iteration 70101 / 84300) loss: 129128370536.902863\n",
      "(Iteration 70201 / 84300) loss: 132817242230.660492\n",
      "(Iteration 70301 / 84300) loss: 132753171467.773285\n",
      "(Iteration 70401 / 84300) loss: 130753337351.228180\n",
      "(Iteration 70501 / 84300) loss: 135466597627.110840\n",
      "(Iteration 70601 / 84300) loss: 134285452357.982849\n",
      "(Iteration 70701 / 84300) loss: 133292747426.407120\n",
      "(Iteration 70801 / 84300) loss: 133036466663.395966\n",
      "(Epoch 84 / 100) train acc: 0.115000; val_acc: 0.117333\n",
      "(Iteration 70901 / 84300) loss: 132613152093.434418\n",
      "(Iteration 71001 / 84300) loss: 136513451357.947433\n",
      "(Iteration 71101 / 84300) loss: 133353274976.478867\n",
      "(Iteration 71201 / 84300) loss: 133621601766.156586\n",
      "(Iteration 71301 / 84300) loss: 135106335294.357178\n",
      "(Iteration 71401 / 84300) loss: 132070683518.280518\n",
      "(Iteration 71501 / 84300) loss: 133803233899.779907\n",
      "(Iteration 71601 / 84300) loss: 136847413193.718353\n",
      "(Epoch 85 / 100) train acc: 0.113000; val_acc: 0.117333\n",
      "(Iteration 71701 / 84300) loss: 136025044597.169434\n",
      "(Iteration 71801 / 84300) loss: 135217086512.631287\n",
      "(Iteration 71901 / 84300) loss: 143459709978.117462\n",
      "(Iteration 72001 / 84300) loss: 138701957527.301575\n",
      "(Iteration 72101 / 84300) loss: 141265471646.103394\n",
      "(Iteration 72201 / 84300) loss: 136009862461.367706\n",
      "(Iteration 72301 / 84300) loss: 141957603754.262360\n",
      "(Iteration 72401 / 84300) loss: 139347483467.209229\n",
      "(Epoch 86 / 100) train acc: 0.112000; val_acc: 0.117333\n",
      "(Iteration 72501 / 84300) loss: 141267580109.788971\n",
      "(Iteration 72601 / 84300) loss: 143631460217.817078\n",
      "(Iteration 72701 / 84300) loss: 144672518786.562744\n",
      "(Iteration 72801 / 84300) loss: 146774912521.269226\n",
      "(Iteration 72901 / 84300) loss: 143987470293.198944\n",
      "(Iteration 73001 / 84300) loss: 141530031176.542358\n",
      "(Iteration 73101 / 84300) loss: 139675955882.681824\n",
      "(Iteration 73201 / 84300) loss: 142435578053.308594\n",
      "(Iteration 73301 / 84300) loss: 142731847425.811829\n",
      "(Epoch 87 / 100) train acc: 0.113000; val_acc: 0.117333\n",
      "(Iteration 73401 / 84300) loss: 146298332207.275574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 73501 / 84300) loss: 145067573133.774292\n",
      "(Iteration 73601 / 84300) loss: 144220621124.377625\n",
      "(Iteration 73701 / 84300) loss: 148237177559.542664\n",
      "(Iteration 73801 / 84300) loss: 147700672108.989838\n",
      "(Iteration 73901 / 84300) loss: 144743345976.487579\n",
      "(Iteration 74001 / 84300) loss: 146770132830.192505\n",
      "(Iteration 74101 / 84300) loss: 149071772741.924133\n",
      "(Epoch 88 / 100) train acc: 0.108000; val_acc: 0.117333\n",
      "(Iteration 74201 / 84300) loss: 145835262788.168030\n",
      "(Iteration 74301 / 84300) loss: 143456838632.971252\n",
      "(Iteration 74401 / 84300) loss: 150438526768.124084\n",
      "(Iteration 74501 / 84300) loss: 147089773744.769623\n",
      "(Iteration 74601 / 84300) loss: 147200983517.056946\n",
      "(Iteration 74701 / 84300) loss: 150756316968.691162\n",
      "(Iteration 74801 / 84300) loss: 152889426337.953217\n",
      "(Iteration 74901 / 84300) loss: 148817249161.325317\n",
      "(Iteration 75001 / 84300) loss: 154757979464.159363\n",
      "(Epoch 89 / 100) train acc: 0.125000; val_acc: 0.117333\n",
      "(Iteration 75101 / 84300) loss: 148213673607.355804\n",
      "(Iteration 75201 / 84300) loss: 152888038466.478271\n",
      "(Iteration 75301 / 84300) loss: 152101216506.662354\n",
      "(Iteration 75401 / 84300) loss: 150795327626.994446\n",
      "(Iteration 75501 / 84300) loss: 153261157132.868835\n",
      "(Iteration 75601 / 84300) loss: 151955548466.516479\n",
      "(Iteration 75701 / 84300) loss: 156768782615.412781\n",
      "(Iteration 75801 / 84300) loss: 152688646930.724792\n",
      "(Epoch 90 / 100) train acc: 0.098000; val_acc: 0.117333\n",
      "(Iteration 75901 / 84300) loss: 156732341444.443695\n",
      "(Iteration 76001 / 84300) loss: 154980400122.251038\n",
      "(Iteration 76101 / 84300) loss: 152212634124.946045\n",
      "(Iteration 76201 / 84300) loss: 155977158110.643768\n",
      "(Iteration 76301 / 84300) loss: 154491377183.877563\n",
      "(Iteration 76401 / 84300) loss: 155028090350.956696\n",
      "(Iteration 76501 / 84300) loss: 151390798878.357361\n",
      "(Iteration 76601 / 84300) loss: 156738320649.045593\n",
      "(Iteration 76701 / 84300) loss: 156415514327.689270\n",
      "(Epoch 91 / 100) train acc: 0.109000; val_acc: 0.117333\n",
      "(Iteration 76801 / 84300) loss: 158579914645.547302\n",
      "(Iteration 76901 / 84300) loss: 157120350238.413635\n",
      "(Iteration 77001 / 84300) loss: 152155390633.081573\n",
      "(Iteration 77101 / 84300) loss: 163315018761.093567\n",
      "(Iteration 77201 / 84300) loss: 159864550202.616028\n",
      "(Iteration 77301 / 84300) loss: 160103549405.959839\n",
      "(Iteration 77401 / 84300) loss: 157347413432.874115\n",
      "(Iteration 77501 / 84300) loss: 161784426883.827026\n",
      "(Epoch 92 / 100) train acc: 0.101000; val_acc: 0.117333\n",
      "(Iteration 77601 / 84300) loss: 159388673066.800354\n",
      "(Iteration 77701 / 84300) loss: 163201501139.391022\n",
      "(Iteration 77801 / 84300) loss: 162529267099.450073\n",
      "(Iteration 77901 / 84300) loss: 165684866388.766785\n",
      "(Iteration 78001 / 84300) loss: 161934417189.737213\n",
      "(Iteration 78101 / 84300) loss: 160450313046.384338\n",
      "(Iteration 78201 / 84300) loss: 161379427567.742340\n",
      "(Iteration 78301 / 84300) loss: 162528454813.062439\n",
      "(Epoch 93 / 100) train acc: 0.128000; val_acc: 0.117333\n",
      "(Iteration 78401 / 84300) loss: 169443153051.792908\n",
      "(Iteration 78501 / 84300) loss: 163140103652.410156\n",
      "(Iteration 78601 / 84300) loss: 162276588372.137451\n",
      "(Iteration 78701 / 84300) loss: 169721462559.237122\n",
      "(Iteration 78801 / 84300) loss: 158824598981.766418\n",
      "(Iteration 78901 / 84300) loss: 162125500527.929260\n",
      "(Iteration 79001 / 84300) loss: 167536932586.721161\n",
      "(Iteration 79101 / 84300) loss: 167366653602.943359\n",
      "(Iteration 79201 / 84300) loss: 163113665793.087982\n",
      "(Epoch 94 / 100) train acc: 0.124000; val_acc: 0.117333\n",
      "(Iteration 79301 / 84300) loss: 167311919715.169800\n",
      "(Iteration 79401 / 84300) loss: 171750835830.483337\n",
      "(Iteration 79501 / 84300) loss: 167182767995.087128\n",
      "(Iteration 79601 / 84300) loss: 167484545326.275269\n",
      "(Iteration 79701 / 84300) loss: 172009876352.842865\n",
      "(Iteration 79801 / 84300) loss: 171523852456.199219\n",
      "(Iteration 79901 / 84300) loss: 175033721915.236694\n",
      "(Iteration 80001 / 84300) loss: 170066577171.874329\n",
      "(Epoch 95 / 100) train acc: 0.086000; val_acc: 0.117333\n",
      "(Iteration 80101 / 84300) loss: 167627200938.921143\n",
      "(Iteration 80201 / 84300) loss: 167249602677.568848\n",
      "(Iteration 80301 / 84300) loss: 170964593820.542786\n",
      "(Iteration 80401 / 84300) loss: 175000731028.024506\n",
      "(Iteration 80501 / 84300) loss: 171619612826.135437\n",
      "(Iteration 80601 / 84300) loss: 173950677214.085968\n",
      "(Iteration 80701 / 84300) loss: 173764915507.814789\n",
      "(Iteration 80801 / 84300) loss: 173723820025.283997\n",
      "(Iteration 80901 / 84300) loss: 175307407874.105377\n",
      "(Epoch 96 / 100) train acc: 0.099000; val_acc: 0.117333\n",
      "(Iteration 81001 / 84300) loss: 176496786484.323090\n",
      "(Iteration 81101 / 84300) loss: 173440333344.739746\n",
      "(Iteration 81201 / 84300) loss: 178902573274.868866\n",
      "(Iteration 81301 / 84300) loss: 176393180636.395508\n",
      "(Iteration 81401 / 84300) loss: 177641348365.302734\n",
      "(Iteration 81501 / 84300) loss: 177624028870.076782\n",
      "(Iteration 81601 / 84300) loss: 179434518451.742798\n",
      "(Iteration 81701 / 84300) loss: 179843909400.606689\n",
      "(Epoch 97 / 100) train acc: 0.115000; val_acc: 0.117333\n",
      "(Iteration 81801 / 84300) loss: 179014220344.500427\n",
      "(Iteration 81901 / 84300) loss: 178225866909.181427\n",
      "(Iteration 82001 / 84300) loss: 177537654132.698853\n",
      "(Iteration 82101 / 84300) loss: 182866541740.586731\n",
      "(Iteration 82201 / 84300) loss: 181094655009.748566\n",
      "(Iteration 82301 / 84300) loss: 181999512117.705750\n",
      "(Iteration 82401 / 84300) loss: 179505176962.924500\n",
      "(Iteration 82501 / 84300) loss: 180081161240.631409\n",
      "(Iteration 82601 / 84300) loss: 177569195393.841309\n",
      "(Epoch 98 / 100) train acc: 0.109000; val_acc: 0.117333\n",
      "(Iteration 82701 / 84300) loss: 183901144630.790375\n",
      "(Iteration 82801 / 84300) loss: 184891687434.522949\n",
      "(Iteration 82901 / 84300) loss: 180534039870.952271\n",
      "(Iteration 83001 / 84300) loss: 183832718608.129089\n",
      "(Iteration 83101 / 84300) loss: 182923990918.784149\n",
      "(Iteration 83201 / 84300) loss: 187571787444.007965\n",
      "(Iteration 83301 / 84300) loss: 191070205100.354828\n",
      "(Iteration 83401 / 84300) loss: 185365235589.722992\n",
      "(Epoch 99 / 100) train acc: 0.120000; val_acc: 0.117333\n",
      "(Iteration 83501 / 84300) loss: 182832385046.539429\n",
      "(Iteration 83601 / 84300) loss: 191185119365.908508\n",
      "(Iteration 83701 / 84300) loss: 184405291015.525696\n",
      "(Iteration 83801 / 84300) loss: 190746798932.370544\n",
      "(Iteration 83901 / 84300) loss: 187682789321.567505\n",
      "(Iteration 84001 / 84300) loss: 188040116748.701355\n",
      "(Iteration 84101 / 84300) loss: 191003275249.416687\n",
      "(Iteration 84201 / 84300) loss: 188261084615.465698\n",
      "(Epoch 100 / 100) train acc: 0.108000; val_acc: 0.117333\n"
     ]
    }
   ],
   "source": [
    "# fc_relu Classifier - Single Layer\n",
    "import fc_relu\n",
    "fc_relu_model = fc_relu.fc_relu_Classifier(input_dim=d, hidden_dim=None,\n",
    "                 weight_scale=1e-3, reg=0.0)\n",
    "config = {'learning_rate':1e-2}\n",
    "fc_relu_solv = solver.Solver(fc_relu_model, data_mnist, optim_config = config,\n",
    "                  num_epochs=100, batch_size=64, print_every=100)\n",
    "fc_relu_solv.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test accuracy for the single layer Softmax\n",
    "np.sum(np.round(fc_relu_model.loss(X_test)).astype(int) == y_test) / y_test.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc_relu Classifier - Single Layer - sketched\n",
    "import fc_relu\n",
    "fc_relu_model = fc_relu.fc_relu_Classifier(input_dim=d, hidden_dim=None, num_classes=10,\n",
    "                 weight_scale=1e-3, reg=0.0)\n",
    "config = {'learning_rate':1e-2}\n",
    "fc_relu_solv = solver.Solver(fc_relu_model, data_mnist, optim_config = config,\n",
    "                  num_epochs=100, batch_size=64, print_every=100)\n",
    "fc_relu_solv.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test accuracy for the single layer Softmax\n",
    "test_acc = np.sum(np.argmax(softmax_model.loss(X_test),axis=1) == y_test) / len(y_test)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax Classifier - Double Layer\n",
    "import softmax\n",
    "softmax_double_model = softmax.SoftmaxClassifier(input_dim=d, hidden_dim=500, num_classes=10,\n",
    "                 weight_scale=1e-3, reg=0.0)\n",
    "config = {'learning_rate':1e-2}\n",
    "softmax_double_solv = solver.Solver(softmax_double_model, data_mnist, optim_config = config,\n",
    "                  num_epochs=100, batch_size=64, print_every=1000)\n",
    "softmax_double_solv.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test accuracy for the single layer Softmax\n",
    "test_acc = np.sum(np.argmax(softmax_double_model.loss(X_test),axis=1) == y_test) / len(y_test)\n",
    "test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load MNIST Data\n",
    "import numpy as np\n",
    "from layers import *\n",
    "import solver\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                   (0.5, 0.5), (0.5, 0.5))\n",
    "                             ])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=60000, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(trainset, batch_size=10000, shuffle=True)\n",
    "\n",
    "classes = ('0', '1', '2', '3',\n",
    "           '4', '5', '6', '7', '8', '9')\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "X_train_full, y_train_full = dataiter.next()\n",
    "X_train_full = X_train_full.numpy()\n",
    "y_train_full = y_train_full.numpy()\n",
    "\n",
    "n_train = int(np.round(0.9*X_train_full.shape[0]))\n",
    "\n",
    "X_train = X_train_full[:n_train]\n",
    "X_val = X_train_full[n_train:]\n",
    "y_train = y_train_full[:n_train]\n",
    "y_val = y_train_full[n_train:]\n",
    "data_mnist = {'X_train':X_train, 'X_val':X_val, 'y_train':y_train, 'y_val':y_val}                              \n",
    "                              \n",
    "dataiter = iter(testloader)\n",
    "X_test, y_test = dataiter.next()\n",
    "X_test = X_test.numpy()\n",
    "y_test = y_test.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN - no batch norm or dropout\n",
    "import cnn\n",
    "d = X_train_full[1].shape\n",
    "cnn_model = cnn.ConvNet(input_dim=d, num_filters=8, filter_size=3, \\\n",
    "            hidden_dim=100, num_classes=10, weight_scale=1e-3, reg=0.0, dtype=np.float32)\n",
    "config = {'learning_rate':1e-2}\n",
    "cnn_solv = solver.Solver(cnn_model, data_mnist, optim_config = config,\n",
    "                  num_epochs=5, batch_size=64, print_every=2)\n",
    "cnn_solv.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
