{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newton Sketch: A Linear-time Optimization Algorithm with Linear-Quadratic Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton Sketch Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import fjlt\n",
    "from scipy.optimize import minimize\n",
    "import time\n",
    "\n",
    "def Newton_Sketch(f, grad, Hess_sr, x0, m, SketchType='SubSamp'):\n",
    "    # algorithm parameters\n",
    "    tol = 1e-6\n",
    "    maxiter = 1000\n",
    "\n",
    "    \n",
    "    #initialize quantities and note sizes of quantities\n",
    "    d = x0.size\n",
    "    H_sr = Hess_sr(x0);\n",
    "    n = H_sr.shape[0]\n",
    "        \n",
    "    #solve the newton-sketch problem\n",
    "    dx = 100 #dummy value to start iterating\n",
    "    xs = np.zeros((x0.size, maxiter+1))\n",
    "    xs[:,0] = x0\n",
    "    t0 = time.time()\n",
    "    times = np.zeros((1, maxiter+1))\n",
    "    numiter = 0\n",
    "    x = x0\n",
    "    for i in range(maxiter):\n",
    "        #print(numiter)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #define S and B = S*H_sr\n",
    "        H_sr = Hess_sr(x);\n",
    "        g = grad(x)\n",
    "        if SketchType is 'Gaussian':\n",
    "            S = np.random.randn(m, n)\n",
    "            B = S @ H_sr\n",
    "        if SketchType is 'SubSamp':\n",
    "            ind = np.random.permutation(n)[:m]\n",
    "            B = H_sr[ind, :]\n",
    "            \n",
    "        #find the approximate newton direction\n",
    "        \n",
    "        \n",
    "        subprob = lambda v: g.T@v + 0.5*np.sum(np.power(B@v, 2))\n",
    "        subgrad = lambda v: g + B.T@(B@v)\n",
    "        res = minimize(subprob, x, jac=subgrad, method=\"CG\", \\\n",
    "                      options={'gtol': 1e-3, 'norm': 2.0, 'eps': 0.1, \\\n",
    "                                       'maxiter': None, 'disp': False})\n",
    "        dx = res.x\n",
    "        \n",
    "        decr = np.sum(dx*g)\n",
    "        \n",
    "        #return if epsilon suboptimal\n",
    "        if (decr**2) <= 2*tol:\n",
    "            xstar = x\n",
    "            opt_gaps = np.apply_along_axis(f, 0,  (xs[:, :numiter+1])) - f(xstar)\n",
    "            return xstar, opt_gaps, times[:,:numiter+1], numiter, xs\n",
    "        \n",
    "        #line search\n",
    "        u = 0.2\n",
    "        fval = f(x)\n",
    "        while f(x + u*dx) > fval + 0.25*u*decr:\n",
    "            u = 0.95*u\n",
    "        #print(u)\n",
    "        # update and keep track of everything\n",
    "        times[:,i+1] = time.time() - t0\n",
    "        x = x + u*dx\n",
    "        xs[:,i+1] = x\n",
    "        numiter += 1\n",
    "        \n",
    "    xstar = x\n",
    "    opt_gaps = np.apply_along_axis(f, 0,  (xs[:, :numiter+1])) - f(xstar)\n",
    "    return xstar, opt_gaps, times[:,:numiter+1], numiter, xs\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Newton(f, grad, Hess_sr, x0):\n",
    "    # algorithm parameters\n",
    "    tol = 1e-6\n",
    "    maxiter = 1000\n",
    "\n",
    "    \n",
    "    #initialize quantities and note sizes of quantities\n",
    "    d = x0.size\n",
    "    H_sr = Hess_sr(x0);\n",
    "    n = H_sr.shape[0]\n",
    "        \n",
    "    #solve the newton-sketch problem\n",
    "    dx = 100 #dummy value to start iterating\n",
    "    xs = np.zeros((x0.size, maxiter+1))\n",
    "    xs[:,0] = x0\n",
    "    t0 = time.time()\n",
    "    times = np.zeros((1, maxiter+1))\n",
    "    numiter = 0\n",
    "    x = x0\n",
    "    for i in range(maxiter):\n",
    "        #print(numiter)\n",
    "        \n",
    "        #find the approximate newton direction\n",
    "        H_sr = Hess_sr(x);\n",
    "        g = grad(x)\n",
    "        \n",
    "        subprob = lambda v: g.T@v + 0.5*np.sum(np.power(H_sr@v, 2))\n",
    "        subgrad = lambda v: g + H_sr.T@(H_sr@v)\n",
    "        res = minimize(subprob, x, jac=subgrad, method=\"BFGS\", \\\n",
    "                      options={'gtol': 1e-3, 'norm': 2.0, 'eps': 0.1, \\\n",
    "                                       'maxiter': None, 'disp': False})\n",
    "        dx = res.x\n",
    "        \n",
    "        decr = np.sum(dx*g)\n",
    "        \n",
    "        #return if epsilon suboptimal\n",
    "        if (decr**2) <= 2*tol:\n",
    "            xstar = x\n",
    "            opt_gaps = np.apply_along_axis(f, 0,  (xs[:, :numiter+1])) - f(xstar)\n",
    "            return xstar, opt_gaps, times[:,:numiter+1], numiter, xs\n",
    "        \n",
    "        #line search\n",
    "        u = 1\n",
    "        fval = f(x)\n",
    "        while f(x + u*dx) > fval + 0.25*u*decr:\n",
    "            u = 0.95*u\n",
    "        #print(u)\n",
    "        # update and keep track of everything\n",
    "        times[:,i+1] = time.time() - t0\n",
    "        x = x + u*dx\n",
    "        xs[:,i+1] = x\n",
    "        numiter += 1\n",
    "        \n",
    "    xstar = x\n",
    "    opt_gaps = np.apply_along_axis(f, 0,  (xs[:, :numiter+1])) - f(xstar)\n",
    "    return xstar, opt_gaps, times[:,:numiter+1], numiter, xs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(f, grad, x0):\n",
    "    # algorithm parameters\n",
    "    tol = 1e-5\n",
    "    maxiter = 1000\n",
    "    \n",
    "    #initialize quantities and note sizes of quantities\n",
    "    d = x0.size\n",
    "        \n",
    "    #solve the newton-sketch problem\n",
    "    dx = 100 #dummy value to start iterating\n",
    "    xs = np.zeros((x0.size, maxiter+1))\n",
    "    xs[:,0] = x0\n",
    "    t0 = time.time()\n",
    "    times = np.zeros((1, maxiter+1))\n",
    "    numiter = 0\n",
    "    x = x0\n",
    "    for i in range(maxiter):\n",
    "        #print(numiter)\n",
    "        \n",
    "        #find the descent direction\n",
    "        g = grad(x)\n",
    "        dx = -g\n",
    "        \n",
    "        #return if epsilon suboptimal\n",
    "        if np.sum(np.power(dx,2)) <= tol:\n",
    "            xstar = x\n",
    "            opt_gaps = np.apply_along_axis(f, 0,  (xs[:, :numiter+1])) - f(xstar)\n",
    "            return xstar, opt_gaps, times[:,:numiter+1], numiter, xs\n",
    "        \n",
    "        #line search\n",
    "        u = 1\n",
    "        fval = f(x)\n",
    "        while f(x + u*dx) > fval + 0.25*u*np.dot(dx,g):\n",
    "            u = 0.95*u\n",
    "        #print(u)\n",
    "        # update and keep track of everything\n",
    "        times[:,i+1] = time.time() - t0\n",
    "        x = x + u*dx\n",
    "        xs[:,i+1] = x\n",
    "        numiter += 1\n",
    "        \n",
    "    xstar = x\n",
    "    opt_gaps = np.apply_along_axis(f, 0,  (xs[:, :numiter+1])) - f(xstar)\n",
    "    return xstar, opt_gaps, times[:,:numiter+1], numiter, xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define cost, grad, hess square root for least squares\n",
    "import numpy as np\n",
    "\n",
    "n = 2000\n",
    "d = 500\n",
    "m = d\n",
    "\n",
    "A = 3*np.random.rand(n, d)\n",
    "b = A@np.random.rand(d) + 0.5*np.random.randn(n)\n",
    "x0 = 5*np.ones(d)\n",
    "\n",
    "#least squares cost\n",
    "def ls(x):\n",
    "    return np.sum(np.power(A@x-b, 2))\n",
    "\n",
    "#least squares gradient\n",
    "def lsgrad(x):\n",
    "    return A.T@(A@x-b)\n",
    "\n",
    "#least squares sqrt hessian\n",
    "def lshess_sr(x):\n",
    "    return A\n",
    "\n",
    "x_opt, _, _, _ = np.linalg.lstsq(A, b, rcond=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xstar_ns, gaps_ns, times_ns, numiter_ns, xs_ns = Newton_Sketch(ls, lsgrad, \\\n",
    "                                                               lshess_sr, x0, int(0.55*n), SketchType='SubSamp')\n",
    "print(times_ns[:,-1], numiter_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xstar_nt, gaps_nt, times_nt, numiter_nt, xs_nt = Newton(ls, lsgrad, lshess_sr, x0)\n",
    "print(times_nt[:,-1], numiter_nt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xstar_gd, opt_gaps_gd, times_gd, numiter_gd, xs_gd = gd(ls, lsgrad, x0)\n",
    "print(times_gd[:,-1], numiter_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaps_gd = np.apply_along_axis(ls, 0, xs_gd) - ls(x_opt)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot( times_ns.flatten(), gaps_ns + 1e-5, 'r^',  times_nt.flatten(), gaps_nt+ 1e-5, 'bs', \\\n",
    "         times_gd.flatten()[:-1], opt_gaps_gd[:-1] + 1e-5, 'g--')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.legend(['Newton-Sketch', 'Newton', 'GD'])\n",
    "plt.title('Optimality Gap vs Wall Clock Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaps_gd = np.apply_along_axis(ls, 0, xs_gd) - ls(x_opt)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot( range(numiter_ns+1), gaps_ns + 1e-5, 'r^',  range(numiter_nt+1), gaps_nt+ 1e-5, 'bs', \\\n",
    "         range(numiter_gd), opt_gaps_gd[:-1] + 1e-5, 'g--')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.legend(['Newton-Sketch', 'Newton', 'GD'])\n",
    "plt.title('Optimality Gap vs Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define cost, grad, hess square root for logistic regression\n",
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "import time\n",
    "from scipy.optimize import *\n",
    "\n",
    "n = 2000\n",
    "d = 100\n",
    "m = d\n",
    "\n",
    "A = np.random.rand(n, d+1)\n",
    "#A[int(n/2):, :] = np.random.rand(int(n/2), d+1) + 0.1\n",
    "A[:, 0] = 1; #for intercept\n",
    "b = np.round(np.sum(A + 0.9*np.random.rand(n, d+1) + 2, axis=1))\n",
    "x0 = np.zeros(d+1)\n",
    "\n",
    "#random convex cost\n",
    "def ps(x):\n",
    "    return np.sum(np.exp(b * (A@x)))\n",
    "\n",
    "#least squares gradient\n",
    "def psgrad(x):\n",
    "    return np.sum((A.T * b * np.exp(b * (A@x))).T, axis=0)\n",
    "\n",
    "#least squares sqrt hessian\n",
    "def pshess_sr(x):\n",
    "    return ((np.exp(b * (A@x))**0.5) * A.T).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xstar_ns, gaps_ns, times_ns, numiter_ns, xs_ns = Newton_Sketch(ps, psgrad, pshess_sr, \\\n",
    "                                                               x0, int(0.85*n), SketchType='SubSamp')\n",
    "#xstar_ns, gaps_ns, times_ns, numiter_ns = Newton_Sketch(ps, psgrad, pshess_sr, x0, int(0.6*n), SketchType='Gaussian')\n",
    "print(times_ns[:,-1], numiter_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xstar_nt, gaps_nt, times_nt, numiter_nt, xs_nt = Newton(ps, psgrad, pshess_sr, x0)\n",
    "print(times_nt[:,-1], numiter_nt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xstar_gd, opt_gaps_gd, times_gd, numiter_gd, xs_gd = gd(ps, psgrad, x0)\n",
    "print(times_gd[:,-1], numiter_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaps_gd = np.apply_along_axis(ls, 0, xs_gd) - ls(xstar_nt)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot( times_ns.flatten(), gaps_ns + 1e-5, 'r^',  times_nt.flatten(), gaps_nt+ 1e-5, 'bs', \\\n",
    "         times_gd.flatten()[:-1], opt_gaps_gd[:-1] + 1e-5, 'g--')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.legend(['Newton-Sketch', 'Newton', 'GD'])\n",
    "plt.title('Optimality Gap vs Wall Clock Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaps_gd = np.apply_along_axis(ls, 0, xs_gd) - ls(xstar_nt)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot( range(numiter_ns+1), gaps_ns + 1e-5, 'r^',  range(numiter_nt+1), gaps_nt+ 1e-5, 'bs', \\\n",
    "         range(numiter_gd), opt_gaps_gd[:-1] + 1e-5, 'g--')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.legend(['Newton-Sketch', 'Newton', 'GD'])\n",
    "plt.title('Optimality Gap vs Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
