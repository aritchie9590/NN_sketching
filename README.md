# NN_sketching

### Papers

#### Sketching
1. [Iterative Hessian Sketch: Fast and Accurate SolutionApproximation for Constrained Least-Squares](http://www.jmlr.org/papers/volume17/14-460/14-460.pdf) M. Pilanci, M. J. Wainwright, JMLR 2016
2. [Information-TheoreticMethods in Data Science: Information-theoretic bounds on sketching](http://web.stanford.edu/~pilanci/papers/infosketch.pdf) M. Pilanci, Stanford

#### Second-Order optimization methods
1. [Training Feedforward Networks with the  Marquardt Algorithm ](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=329697) M. T. Hagan, M. B. Menhaj, IEEE Transactions on Neural Networks 1994
2. [Fast Curvature Matrix-Vector Products for Second-Order Gradient Descent](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=0258AD1C6C96F0DC0BCB0F456F5BCF88?doi=10.1.1.421.1443&rep=rep1&type=pdf) N. N. Schraudolph, Neural Computation 2002
3. [Practical Gauss-Newton Optimisation for Deep Learning](https://arxiv.org/pdf/1706.03662.pdf) A. Botev, H. Ritter, D. Barber, ICML 2017
4. [Deep learning via Hessian-free optimization](http://www.cs.toronto.edu/~jmartens/docs/Deep_HessianFree.pdf) J. Martens, ICML 2010

### Weblinks

#### Jacobian-Vector products
1. [A new trick for calculating Jacobian vector products](https://j-towns.github.io/2017/06/12/A-new-trick.html)
2. [Automatic Differentiation](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf)
3. [PyTorch Autograd](https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95)
4. [Computing the Jacobian matrix of a neural network in Python](https://medium.com/unit8-machine-learning-publication/computing-the-jacobian-matrix-of-a-neural-network-in-python-4f162e5db180)
